{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZ8wEbZnBnQ"
   },
   "source": [
    "**Project 1 Phase 2**<br>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Michael</td>\n",
    "    <td style = \"text-align: left\">Weikl</td>\n",
    "    <td style = \"text-align: left\">1154652</td>\n",
    "  </tr>\n",
    "\n",
    "  \n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Problemdefinition](#Problemdefinition)\n",
    "* [Basic functions](#Basic-functions)\n",
    "    * [Stopping Criterion](#Stopping-Criterion)\n",
    "    * [Backtracking](#Backtracking)\n",
    "    * [Wolfe Condition](#Wolfe-Condition)\n",
    "    * [Derivation (Taylor-Theorem)](Derivation-(Taylor-Theorem))\n",
    "* [Implementation of the algorithms](#Implementation-of-the-algorithms)\n",
    "    * [Steepest descent](#Steepest-descent)\n",
    "    * [Newthon method](#Newthon-method)\n",
    "    * [Fetcher Reeves](#Fetcher-Reeves)\n",
    "    * [SR1](#SR1)\n",
    "* [Executions](#Executions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KwrOzH2PmHIH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemdefinition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqP_WueRyEc-"
   },
   "source": [
    "Basic Problem class to set up problems for the algorithms to be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eNaoAtoPmPM-"
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "    def f(self, x):\n",
    "        raise NotImplementedError(\"f() is not implemented for this Problem\")\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        raise NotImplementedError(\"grad_f() is not implemented for this Problem\")\n",
    "            \n",
    "    def hessian(self, x):\n",
    "        raise NotImplementedError(\"hessian() is not implemented for this Problem\")      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " class PolynomialProblem(Problem):\n",
    "        @property\n",
    "        def actual_minima(self):\n",
    "            raise NotImplementedError(\"actual_minima() is not implemented for this Problem\")      \n",
    "\n",
    "\n",
    "############################################################\n",
    "#                Polynomial definitions\n",
    "############################################################\n",
    "class Himmelblau(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                         4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian(self, x):\n",
    "        return np.array([[12*x[0]**2 + 4*x[1] - 42, \n",
    "                          4*(x[1] + x[0])],\n",
    "                         [4*(x[1] + x[0]),\n",
    "                          12*x[1]**2 + 4*x[0] - 26]])\n",
    "    \n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([[3,2], \n",
    "                         [-2.805118, 3.131312], \n",
    "                         [-3.779310, -3.283186], \n",
    "                         [3.584428, -1.848126]])\n",
    "\n",
    "    \n",
    "class Polynomial1(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian(self, x):\n",
    "        return 3 * x**2 - 30 * x + 71\n",
    "    \n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([[3],[5],[7]])\n",
    "\n",
    "    \n",
    "class Rosenbrock(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(self, x):\n",
    "        return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "                         200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian(self, x):\n",
    "        return np.array([[-400*(x[1] - 3*x[0]**2) + 2, \n",
    "                          -400*x[0]],\n",
    "                         [-400*x[0], \n",
    "                          200]])\n",
    "    \n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([1,1])\n",
    "\n",
    "\n",
    "#this is the function below rosenbrock defined in the project description\n",
    "class Function2(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return 150*(x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        return np.array([x[0] * (300 * x[1]**2 + 0.5) + 2 * x[1] - 2, 300*x[0]**2 * x[1] + 2 * x[0] + 8 * x[1] - 8])\n",
    "\n",
    "    def hessian(self, x):\n",
    "        return np.array([[300 * x[1]**2 + 0.5, 600 * x[0]*x[1] + 2],[600 * x[0]*x[1] + 2, 300 * x[0]**2 + 8]])\n",
    "\n",
    "    #self calculated minima\n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([[0,1],[4,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for squared error problems\n",
    "def normal_distributed_data_points(target_function, q: float, num_points: int):\n",
    "    # bring normal distributed value from range [0, 1] to  [-q, q]\n",
    "    return distributed_data_points(target_function=target_function, q=q, num_points=num_points,\n",
    "                                   distribution=lambda r: (np.random.normal(0, scale=2 * r) - r))\n",
    "\n",
    "\n",
    "def uniform_distributed_data_points(target_function, q: float, num_points: int):\n",
    "    return distributed_data_points(target_function=target_function, q=q, num_points=num_points,\n",
    "                                   distribution=lambda r: np.random.uniform(low=-r, high=r))\n",
    "\n",
    "\n",
    "def distributed_data_points(target_function, q: float, num_points: int, distribution):\n",
    "    result_list = []\n",
    "\n",
    "    for _ in range(num_points):\n",
    "        val = distribution(abs(q))\n",
    "        result_list.append((val, target_function(val)))\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredErrorProblem(Problem):\n",
    "    def __init__(self, target_function,\n",
    "                 q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        if distribution == 'uniform':\n",
    "            self.data = uniform_distributed_data_points(target_function=target_function, q=q, num_points=num_points)\n",
    "        elif distribution == 'normal':\n",
    "            self.data = normal_distributed_data_points(target_function=target_function, q=q, num_points=num_points)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Functionality not implemented for ditribution type '{distribution}'. Use 'uniform' or 'normal'.\")\n",
    "\n",
    "        self.coefficients = []\n",
    "        for j in range(num_points):\n",
    "            a_j = self.data[j][0]\n",
    "            c_j = np.reshape([a_j ** i for i in range(degree + 1)], (degree + 1, 1))\n",
    "            self.coefficients.append(c_j)\n",
    "\n",
    "        self.target_function = target_function\n",
    "\n",
    "    def __residual(self, x, j):\n",
    "        a_j = self.data[j][0]\n",
    "        b_j = self.data[j][1]\n",
    "        c_j = self.coefficients[j]\n",
    "\n",
    "        return c_j.T @ x - b_j\n",
    "\n",
    "    def f(self, x):\n",
    "        return 1 / 2 * np.sum([self.__residual(x, j) ** 2 for j in range(len(self.data))])\n",
    "\n",
    "    def grad_f(self, x):\n",
    "        return np.sum([self.__residual(x, j) * self.coefficients[j] for j in range(len(self.data))], axis=0)\n",
    "\n",
    "    def hessian(self, x):\n",
    "        return np.sum([self.coefficients[j] @ self.coefficients[j].T for j in range(len(self.data))], axis=0)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#              Squared Error Problem Definition\n",
    "############################################################\n",
    "\n",
    "class SinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: np.sin(x), q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class CosinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: np.cos(x), q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class SinusMinusCosinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: np.sin(x) - np.cos(x), q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class CubePlusSquareSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: x ** 3 + x ** 2, q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class SinusPlusCosinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: 3 * np.sin(x) + np.cos(x), q, num_points, degree, random_seed, distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements a stopping criterion for when the gradient of f at xk is small relative to the gradient of f at x0\n",
    "Also a maxmimum number of iterations is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, current_iteration, tol: float = 1e-8, max_iter: int = 10000):\n",
    "    if current_iteration > max_iter:\n",
    "        return True\n",
    "    elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtracking linesearch to find suitable step length as described in the book. steepest_descent() implements the line search steepest descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
    "    alpha = alpha0\n",
    "    f_xk = f(xk)\n",
    "    grad_f_xk = grad_f(xk)\n",
    "\n",
    "    while f(xk + alpha * pk) > (f_xk + c * alpha * grad_f_xk.T @ pk):\n",
    "        alpha *= rho\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wolfe Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf. newton_method() implements the line search Newton method solving the equation H @ pk = -grad instead of computing the inverse of H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
    "    alpha = 0\n",
    "    beta = np.Inf\n",
    "    t = 1\n",
    "\n",
    "    while True:\n",
    "        if f(xk + t * pk) > (f(xk) + c1 * t * (pk.T @ grad_f(xk))):\n",
    "            beta = t\n",
    "            t = 0.5 * (alpha + beta)\n",
    "        elif (-pk.T @ grad_f(xk + t * pk)) > (-c2 * pk.T @ grad_f(xk)):\n",
    "            alpha = t\n",
    "            t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
    "        else:\n",
    "            return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation (Taylor Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements the forward difference approach from the book for gradient and hessian calculation without explicit function knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "    arr = np.zeros(size)\n",
    "    arr[index] = 1.0\n",
    "    return arr\n",
    "\n",
    "\n",
    "# these return the function that computes the gradient, which can then in turn be called to compute the gradient\n",
    "def approx_grad(f, e=1.1e-8):\n",
    "    def grad_f(x):\n",
    "        if x.size == 1:\n",
    "            return (f(x + e) - f(x)) / e\n",
    "        return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "\n",
    "    return grad_f\n",
    "\n",
    "\n",
    "def approx_hessian(f, e=1.1e-8):\n",
    "    def hessian_f(x):\n",
    "        if x.size == 1:\n",
    "            return (f(x + 2 * e) - 2 * f(x + e) + f(x)) / e ** 2\n",
    "        return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "            x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "            x)) / e ** 2 for j in range(x.size)] for i in range(x.size)])\n",
    "\n",
    "    return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(x0, f, grad_f=None, initial_alpha: float = 1e-3, tolerance: float = 1e-8):\n",
    "    if grad_f == None:\n",
    "        grad_f = approx_grad(f)\n",
    "\n",
    "    i = 1\n",
    "    xk = x0\n",
    "    search_alpha, alpha, pk = None, None, None\n",
    "\n",
    "    while not stop_crit(grad_f, xk, x0, i, tol=tolerance):\n",
    "\n",
    "        if search_alpha is None:\n",
    "            search_alpha = initial_alpha\n",
    "            pk = -grad_f(xk)\n",
    "        else:\n",
    "            new_pk = -grad_f(xk)\n",
    "            search_alpha = alpha * (-pk.T @ pk) / (-new_pk.T @ new_pk)\n",
    "            pk = new_pk\n",
    "\n",
    "        alpha = backtracking_alpha(f, grad_f, xk, pk)\n",
    "        xk = xk + alpha * pk\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return xk, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(x0, f, grad_f=None, hessian_f=None, tolerance=1e-8):\n",
    "    if grad_f == None:\n",
    "        grad_f = approx_grad(f)\n",
    "        hessian_f = approx_hessian(f)\n",
    "    i = 1\n",
    "    x = x0\n",
    "\n",
    "    while not stop_crit(grad_f, x, x0, i, tol=tolerance):\n",
    "        pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else solve(hessian_f(x), -grad_f(x))\n",
    "        x = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "        i += 1\n",
    "\n",
    "    #print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetcher Reeves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR(x0, f, grad_f=None):\n",
    "\n",
    "    conv_tol = 1e-8\n",
    "    if grad_f == None:\n",
    "        grad_f = approx_grad(f)\n",
    "    i = 0\n",
    "    xk = x0\n",
    "    pk = -grad_f(xk)\n",
    "\n",
    "    while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
    "\n",
    "        xk1 = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
    "        beta = (grad_f(xk1) @ grad_f(xk1)) / (grad_f(xk) @ grad_f(xk))\n",
    "        pk = -grad_f(xk1) + beta * pk\n",
    "        xk = xk1\n",
    "        i += 1\n",
    " \n",
    "    #print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SR1() implements the line search quasi newton method utilizing SR1 updating for inverse Hessian approximation and a method for stabilizing that resets H as a multiple of I inspired by the method deployed in https://www.sciencedirect.com/science/article/pii/S0898122111004202?via%3Dihub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR1(x0 : np.array, f, grad_f, r=1e-8):\n",
    "\n",
    "    conv_tol = 1e-8\n",
    "    i = 0\n",
    "    H = np.identity(x0.size)\n",
    "    x = x0\n",
    "\n",
    "    while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
    "  \n",
    "        pk = - H @ grad_f(x)\n",
    "        x_1 = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "        sk = x_1 - x\n",
    "        yk = grad_f(x_1) - grad_f(x)\n",
    "        rhok = 1 / (yk.T @ sk)\n",
    "    \n",
    "        if ((sk @ yk - yk @ H @ yk) < 0) or (abs(yk @ (sk - H @ yk)) < r * np.linalg.norm(yk) * np.linalg.norm(sk - H @ yk)) or (\n",
    "            norm(H, np.inf) > 1e10):\n",
    "            mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n",
    "            H_1 = mu * np.identity(x0.size)\n",
    "            H = H_1\n",
    "        else:\n",
    "            H_1 = H + np.outer((sk - H @ yk), (sk - H @ yk)) / ((sk - H @ yk) @ yk)\n",
    "            H = H_1\n",
    "        \n",
    "        x = x_1\n",
    "        i += 1\n",
    "\n",
    "    #print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list with all possible algorithms for the tests\n",
    "algorithms = {'Steepest descent': steepest_descent,\n",
    "              'Newthon method': newton_method,\n",
    "              'Fetcher Reeves': FR,\n",
    "              'SR1': SR1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKXNBar6zlfC",
    "outputId": "b491958b-75af-4e84-fdf7-9255325ce6e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "Problem Himmelblau:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 2816 with result: [-3.77931026 -3.28318599]\n",
      "Last gradient norm: 2.6048138970693457e-07\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3. 2.]' is 8.594829942568188\n",
      "\t->Difference to point '[-2.805118  3.131312]' is 6.488053253235104\n",
      "\t->Difference to point '[-3.77931  -3.283186]' is 2.5521830106979e-07\n",
      "\t->Difference to point '[ 3.584428 -1.848126]' is 7.502268874432074\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Newthon method':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 5001 with result: [-3.49888468e-16 -8.88178420e-16]\n",
      "Last gradient norm: 26.076809620810565\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3. 2.]' is 3.6055512754639905\n",
      "\t->Difference to point '[-2.805118  3.131312]' is 4.20402210213838\n",
      "\t->Difference to point '[-3.77931  -3.283186]' is 5.006245537995115\n",
      "\t->Difference to point '[ 3.584428 -1.848126]' is 4.03282702320097\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Fetcher Reeves':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 502 with result: [ 3.58442834 -1.84812652]\n",
      "Last gradient norm: 2.5406398925923756e-07\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3. 2.]' is 3.892253104483038\n",
      "\t->Difference to point '[-2.805118  3.131312]' is 8.100685799398152\n",
      "\t->Difference to point '[-3.77931  -3.283186]' is 7.502268857235862\n",
      "\t->Difference to point '[ 3.584428 -1.848126]' is 6.237319089753625e-07\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'SR1':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 11 with result: [3. 2.]\n",
      "Last gradient norm: 1.3802086492309621e-07\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3. 2.]' is 4.5270257313018665e-09\n",
      "\t->Difference to point '[-2.805118  3.131312]' is 5.914326825344883\n",
      "\t->Difference to point '[-3.77931  -3.283186]' is 8.594829747657739\n",
      "\t->Difference to point '[ 3.584428 -1.848126]' is 3.892252539435878\n",
      "**********************************************\n",
      "Problem Polynomial1:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 221 with result: [7.00000006]\n",
      "Last gradient norm: 4.627252097937028e-07\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3]' is 4.000000057840649\n",
      "\t->Difference to point '[5]' is 2.0000000578406487\n",
      "\t->Difference to point '[7]' is 5.784064871505734e-08\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Newthon method':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 6 with result: [3.]\n",
      "Last gradient norm: 3.6624925314880115e-11\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3]' is 4.5781156643442955e-12\n",
      "\t->Difference to point '[5]' is 2.000000000004578\n",
      "\t->Difference to point '[7]' is 4.000000000004578\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Fetcher Reeves':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 81 with result: [7.00000005]\n",
      "Last gradient norm: 3.92814385718744e-07\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3]' is 4.000000049101796\n",
      "\t->Difference to point '[5]' is 2.0000000491017964\n",
      "\t->Difference to point '[7]' is 4.910179640660317e-08\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'SR1':\n",
      "------------------------------------------------\n",
      "\n",
      " Search terminated after iteration 1 with result: [7.]\n",
      "Last gradient norm: 0.0\n",
      "Difference to real solutions:\n",
      "\t->Difference to point '[3]' is 4.0\n",
      "\t->Difference to point '[5]' is 2.0\n",
      "\t->Difference to point '[7]' is 0.0\n"
     ]
    }
   ],
   "source": [
    "polynomial_problems = {Himmelblau(): [0,0], \n",
    "                       Polynomial1(): [1]   }\n",
    "\n",
    "for prob, start_value in polynomial_problems.items():\n",
    "    \n",
    "    print(\"**********************************************\")\n",
    "    print(f\"Problem {type(prob).__name__}:\")\n",
    "    print(\"**********************************************\")\n",
    "    for algorithm_name, algorithm in algorithms.items():\n",
    "        \n",
    "        print(\"------------------------------------------------\")\n",
    "        print(f\"\\tAlgorithm '{algorithm_name}':\")\n",
    "        print(\"------------------------------------------------\")\n",
    "        parameters = {'x0': np.array(start_value),\n",
    "                      'f': prob.f,\n",
    "                      'grad_f': prob.grad_f,\n",
    "                      'hessian_f': prob.hessian}\n",
    "        \n",
    "        # clean up parameters for generic method call\n",
    "        possible_parameters = algorithm.__code__.co_varnames\n",
    "        for p in list(parameters):\n",
    "            if p not in possible_parameters:\n",
    "                parameters.pop(p)\n",
    "        \n",
    "        x, iterations = algorithm(**parameters)\n",
    "\n",
    "        print(f\"Search terminated after iteration {iterations} with result: {x}\")\n",
    "        print(f\"Last gradient norm: {norm(prob.grad_f(x))}\")\n",
    "        print(\"Difference to real solutions:\")\n",
    "        for am in prob.actual_minima:\n",
    "            print(f\"\\tDifference to point '{am}' is {norm(x - am)}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# prob = Problem()\n",
    "# prob.poly_1()\n",
    "# print(f\"\\nProblem poly_1: \\n\")\n",
    "# print(\"\\nAlgorithm output exact gradient: \")\n",
    "# x_ = steepest_descent(np.array([1]), prob.f, prob.grad_f)\n",
    "# print(f\"\\nactual minima: {prob.min_x}\")\n",
    "# print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "# print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
    "# print(\"\\nAlgorithm output approximated gradient: \")\n",
    "# x_ = steepest_descent(np.array([1]), prob.f)\n",
    "# print(f\"\\nactual minima: {prob.min_x}\")\n",
    "# grad = approx_grad(prob.f)\n",
    "# print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "# print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared Error Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "Problem SinusSQEP:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-ae2d71a6b6dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Last gradient norm: {norm(prob.grad_f(coef))}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-7417660469ae>\u001b[0m in \u001b[0;36msteepest_descent\u001b[1;34m(x0, f, grad_f)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop_crit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv_tol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mxk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbacktracking_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-e13d07fd64b7>\u001b[0m in \u001b[0;36mbacktracking_alpha\u001b[1;34m(f, grad_f, xk, pk, alpha0, rho, c)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "sqep_problems = {SinusSQEP: {'q': 2, 'num_points': 100, 'degree': 5},\n",
    "                 CosinusSQEP: {'q': 2, 'num_points': 100, 'degree': 5}}\n",
    "\n",
    "for prob_class, prob_params in sqep_problems.items():\n",
    "    \n",
    "    prob = prob_class(**prob_params)\n",
    "\n",
    "    prob_num_points = prob_params['num_points']\n",
    "    prob_q = prob_params['q']\n",
    "    prob_degree = prob_params['degree']\n",
    "\n",
    "    x_plot = [i / 10 for i in range(-prob_q * 10, prob_q * 10 + 1)]\n",
    "    y_true_plot = [prob.target_function(x) for x in x_plot]\n",
    "\n",
    "    print(\"**********************************************\")\n",
    "    print(f\"Problem {type(prob).__name__}:\")\n",
    "    print(\"**********************************************\")\n",
    "    for algorithm_name, algorithm in algorithms.items():\n",
    "\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(f\"\\tAlgorithm '{algorithm_name}':\")\n",
    "        print(\"------------------------------------------------\")\n",
    "\n",
    "        parameters = {'x0': np.reshape([0 for i in range(prob_degree + 1)], (prob_degree + 1, 1)),\n",
    "                      'f': prob.f,\n",
    "                      'grad_f': prob.grad_f,\n",
    "                      'hessian_f': prob.hessian}\n",
    "\n",
    "        # clean up parameters for generic method call\n",
    "        possible_parameters = algorithm.__code__.co_varnames\n",
    "        for p in list(parameters):\n",
    "            if p not in possible_parameters:\n",
    "                parameters.pop(p)\n",
    "\n",
    "        coef, iterations = algorithm(**parameters)\n",
    "        print(f\"Search terminated after iteration {iterations} with result: {coef}\")\n",
    "        print(f\"Last gradient norm: {norm(prob.grad_f(coef))}\")\n",
    "\n",
    "        approximation_y = []\n",
    "        for i in range(len(x_plot)):\n",
    "            approximation_y.append(0)\n",
    "            for d in range(prob_degree):\n",
    "                approximation_y[i] += coef[d] * x_plot[i] ** d\n",
    "\n",
    "        plt.plot(x_plot, y_true_plot, label=\"true function\")\n",
    "        plt.plot(x_plot, approximation_y, '-.', label=\"approximation\")\n",
    "        plt.title(f'{algorithm_name}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# def draw_graphs(degree : int = 5, ):\n",
    "#     x = np.reshape([0 for i in range(degree+1)], (degree+1, 1))\n",
    "\n",
    "#     fig, ax = plt.subplots(4, figsize=(10, 15))\n",
    "\n",
    "#     x_multipliers, steps = newton_descent(x, 1000)\n",
    "#     xs = [i/10 for i in range(-q*10, q*10 + 1)]\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[0].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[0].plot(xs, y_approx,'-.', label=\"approximation\")\n",
    "#     ax[0].set(title=f'Newton method after {steps} steps')\n",
    "#     ax[0].legend()\n",
    "\n",
    "#     x_multipliers, steps = conjugate_gradient(x, 15000)\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[3].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[3].plot(xs, y_approx, label=\"approximation\")\n",
    "#     ax[3].set(title=f'Conjugate gradient method after {steps} steps')\n",
    "#     ax[3].legend()\n",
    "\n",
    "#     x_multipliers, steps = steepest_descent(x, 15000)\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[1].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[1].plot(xs, y_approx, label=\"approximation\")\n",
    "#     ax[1].set(title=f'Steepest descent after {steps} steps')\n",
    "#     ax[1].legend()\n",
    "    \n",
    "#     # converges extremely slowly\n",
    "#     \"\"\"x_multipliers, steps = SR1(x, 15000)\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[2].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[2].plot(xs, y_approx, label=\"approximation\")\n",
    "#     ax[2].set(title=f'SR1 after {steps} steps')\n",
    "#     ax[2].legend()\"\"\"\n",
    "\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "# draw_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code #####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRYmj7uuDWmG"
   },
   "source": [
    "For the newton method I first struggled with solving the himmelblau function, but after some tinkering with the starting point, it worked.\n",
    "Also choosing 0s as starting points seems to not work here at all. In the end I reached VERY fast convergence for all problems though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9Dqi4AWCAp-",
    "outputId": "20d0671e-cc22-49bb-91c1-50c8049596cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 6 with result: [3. 2.]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 4.39446412658135e-14\n",
      "\n",
      "Difference to real solution: 1.4043333874306805e-15\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([2.5,1.5]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rqHHXq9EY3F"
   },
   "source": [
    "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8yFRBxNEitW",
    "outputId": "eea61628-81e1-4b48-9c5f-be39541c2436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 502 with result: [ 3.58442834 -1.84812652]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 2.5406398925923756e-07\n",
      "\n",
      "Difference to real solution: 6.237319089753625e-07\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkZV38tKGrel",
    "outputId": "e5b3ff83-ff39-4eff-b17c-26152a308d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 11 with result: [3. 2.]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 1.3802086492309621e-07\n",
      "\n",
      "Difference to real solution: 4.5270257313018665e-09\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSmu1kaRcFF1"
   },
   "source": [
    "**Rosenbrock [1.2,1.2] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1Skp5VncEqT",
    "outputId": "8fafb866-e85f-4b4a-935d-728c2368f6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem rosenbrock steepest descent [1.2,1.2]:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 5001 with result: [1.00225488 1.00455996]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 0.016306835034657012\n",
      "\n",
      "Difference to real solution: 0.005087017651117713\n",
      "\n",
      "Problem rosenbrock steepest descent [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 5001 with result: [1.00225053 1.00455368]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 0.017387939189828356\n",
      "\n",
      "Difference to real solution: 0.0050794602350955455\n",
      "\n",
      "Problem rosenbrock newton method [1.2,1.2]:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 8 with result: [1. 1.]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.4360392201267428e-11\n",
      "\n",
      "Difference to real solution: 1.7227828323731315e-13\n",
      "\n",
      "Problem rosenbrock newton method [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 7 with result: [0.99999669 0.99999338]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.7793616466006474e-11\n",
      "\n",
      "Difference to real solution: 7.396216495370038e-06\n",
      "\n",
      "Problem rosenbrock FR [1.2,1.2]:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 1031 with result: [1.00000028 1.00000055]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.20863559177819e-06\n",
      "\n",
      "Difference to real solution: 6.199506706177205e-07\n",
      "\n",
      "Problem rosenbrock FR [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 479 with result: [0.99999701 0.99999401]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 3.457225675382256e-07\n",
      "\n",
      "Difference to real solution: 6.699859016039294e-06\n",
      "\n",
      "Problem rosenbrock:\n",
      "\n",
      "Algorithm output SR1 [1.2,1.2]: \n",
      "\n",
      " Search terminated after iteration 24 with result: [1. 1.]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.2700270809299206e-08\n",
      "\n",
      "Difference to real solution: 2.5271779361523775e-09\n",
      "\n",
      "Problem rosenbrock SR1 [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SR1() missing 1 required positional argument: 'grad_f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-964c76aff8dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nAlgorithm output: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSR1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nactual minima: {prob.min_x}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapprox_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: SR1() missing 1 required positional argument: 'grad_f'"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([1.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [1.2,1.2]: \")\n",
    "x_ = SR1(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im-apNvwUEsz"
   },
   "source": [
    "**Rosenbrock [-1.2,1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C0zGoM_s01no",
    "outputId": "6ee66bbe-5959-4d75-a387-6b501cbcf089"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.2,1]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [-1.2,1]: \")\n",
    "x_ = SR1(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [-1.2,1] approximated gradients:\")\n",
    "#for some reason SR1 with this starting point and approximated gradients seems to be running forever\n",
    "\"\"\"print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV80lbwbcjGM"
   },
   "source": [
    "**Rosenbrock [0,1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dmjbZ3ZchH-",
    "outputId": "e546f819-c5f1-4d0a-c3fe-c3485a72bc37"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [0.0,1.0]: \")\n",
    "x_ = SR1(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jiq4MRUJcv2d"
   },
   "source": [
    "**Rosenbrock [-1,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlWrpjIIcwBj",
    "outputId": "db8418b0-98ec-43b5-ca11-e30adc28b8ea"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [-1.0,0.0]: \")\n",
    "x_ = SR1(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyH8r8IIc9hd"
   },
   "source": [
    "**Rosenbrock [0,-1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oInw3wzKc9Xk",
    "outputId": "62e12034-bc92-490a-dc30-efad82307b0f"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [0.0,-1.0]: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivj3CII8dacl"
   },
   "source": [
    "**Function 2 [-0.2,1.2] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvbNrGZKdaS4",
    "outputId": "a72d29a3-f606-4af3-b418-0a6d484b5149"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-0.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkJky8GNdo_N"
   },
   "source": [
    "**Function 2 [3.8,0.1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg-qTessdpI0",
    "outputId": "bd1d28c8-a583-4615-ee7e-b0e526ce2a1b"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([3.8,0.1]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziFsa2GHdzt8"
   },
   "source": [
    "**Function 2 [0,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJG0EmTndz1U",
    "outputId": "2cff35bd-3f79-421a-a726-c86370040edf"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "#This raises linalgerror for singular matrix\n",
    "\"\"\"print(f\"\\nProblem func_2 [0.0,0.0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\"\"\"\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQ1Zg6ZdK6F"
   },
   "source": [
    "**Function 2 [-1,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-m41Z1lwTrSe",
    "outputId": "6fa39d8a-7071-40aa-da33-d92283893947"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1,0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sav7POtNd8v-"
   },
   "source": [
    "**Function 2 [0,-1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4chOrUdBhrVb",
    "outputId": "d40e7a9c-923f-4330-86cd-7d2e4f68c3ee"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "#this raises linalgerror singular matrix\n",
    "\"\"\"print(f\"\\nProblem func_2 [0.0,-1.0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\"\"\"\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOh95c0NmZ2b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project1_phase2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
