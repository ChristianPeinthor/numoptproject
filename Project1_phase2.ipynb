{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kZ8wEbZnBnQ"
      },
      "source": [
        "**Project 1 Phase 2**<br>\n",
        "<br>\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th style = \"text-align: left\">#</th>\n",
        "    <th style = \"text-align: left\">Name</th>\n",
        "    <th style = \"text-align: left\">Lastname</th>\n",
        "    <th style = \"text-align: left\">Matr Number</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">1</td>\n",
        "    <td style = \"text-align: left\">Christian</td>\n",
        "    <td style = \"text-align: left\">Peinthor</td>\n",
        "    <td style = \"text-align: left\">11815592</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">2</td>\n",
        "    <td style = \"text-align: left\">name</td>\n",
        "    <td style = \"text-align: left\">surname</td>\n",
        "    <td style = \"text-align: left\">matnr</td>\n",
        "  </tr>\n",
        "\n",
        "  \n",
        "</table>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KwrOzH2PmHIH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy \n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy.linalg import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqP_WueRyEc-"
      },
      "source": [
        "Problem class to set up problems for the algorithms to be solved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eNaoAtoPmPM-"
      },
      "outputs": [],
      "source": [
        "class Problem():\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.f = None\n",
        "    self.grad_f = None\n",
        "    self.hessian = None\n",
        "    self.min_x = None\n",
        "\n",
        "  def himmelblau(self):\n",
        "\n",
        "    def f(x):\n",
        "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
        "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
        "    \n",
        "    def hessian(x):\n",
        "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
        "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
        "      \n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian = hessian\n",
        "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
        "\n",
        "  def poly_1(self):\n",
        "\n",
        "    def f(x):\n",
        "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return (x - 7) * (x - 5) * (x - 3)\n",
        "    \n",
        "    def hessian(x):\n",
        "      return 3 * x**2 - 30 * x + 71\n",
        "\n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian = hessian\n",
        "    self.min_x = np.array([[3],[5],[7]])\n",
        "\n",
        "  def rosenbrock(self):\n",
        "\n",
        "    def f(x):\n",
        "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "    def grad_f(x):\n",
        "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
        "           200*(x[1] - x[0]**2)])\n",
        "\n",
        "    def hessian(x):\n",
        "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
        "           [-400*x[0], 200]])\n",
        "      \n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian = hessian\n",
        "    self.min_x = np.array([1,1])\n",
        "\n",
        "  #this is the function below rosenbrock defined in the project description\n",
        "  def func_2(self):\n",
        "\n",
        "    def f(x):\n",
        "      return 150*(x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return np.array([x[0] * (300 * x[1]**2 + 0.5) + 2 * x[1] - 2, 300*x[0]**2 * x[1] + 2 * x[0] + 8 * x[1] - 8])\n",
        "\n",
        "    def hessian(x):\n",
        "      return np.array([[300 * x[1]**2 + 0.5, 600 * x[0]*x[1] + 2],[600 * x[0]*x[1] + 2, 300 * x[0]**2 + 8]])\n",
        "\n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian = hessian\n",
        "    #self calculated minima\n",
        "    self.min_x = np.array([[0,1],[4,0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk-SEbLewdKO"
      },
      "source": [
        "**1 Stoppig Criterion**<br>\n",
        "This implements a stopping criterion for when the gradient of f at xk is small relative to the gradient of f at x0\n",
        "Also a maxmimum number of iterations is implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wty1yHWBvGAe"
      },
      "outputs": [],
      "source": [
        "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
        "  if i > max_iter: \n",
        "    return True\n",
        "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
        "    return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 Derivatives**<br>\n",
        "This implements the forward difference approach from the book for gradient and hessian calculation without explicit function knowledge."
      ],
      "metadata": {
        "id": "2zB6LC7FClrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def e_i(size, index):\n",
        "  arr = np.zeros(size)\n",
        "  arr[index] = 1.0\n",
        "  return arr\n",
        "\n",
        "#these return the function that computes the gradient, which can then in turn be called to compute the gradient\n",
        "def approx_grad(f, e=1.1e-8):\n",
        "  def grad_f(x):\n",
        "    if x.size == 1:\n",
        "      return (f(x + e) - f(x)) / e\n",
        "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
        "  return grad_f\n",
        "\n",
        "def approx_hessian(f, e=1.1e-8):\n",
        "  def hessian_f(x):\n",
        "    if x.size == 1:\n",
        "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
        "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
        "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
        "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
        "  return hessian_f"
      ],
      "metadata": {
        "id": "bQW9iyomCkUF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3AH9rXrynuf"
      },
      "source": [
        "Backtracking linesearch to find suitable step length as described in the book. steepest_descent() implements the line search steepest descent method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A9qu-9xzwcEQ"
      },
      "outputs": [],
      "source": [
        "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
        "\n",
        "  alpha = alpha0\n",
        "\n",
        "  while not f(xk + alpha * pk) <= (f(xk) + c * alpha * grad_f(xk).T @ pk):\n",
        "    alpha *= rho\n",
        "  \n",
        "  return alpha\n",
        "\n",
        "def steepest_descent(x0, f, grad_f=None):\n",
        "\n",
        "  conv_tol = 1e-8\n",
        "  if grad_f == None:\n",
        "    grad_f = approx_grad(f)\n",
        "  i = 0\n",
        "  xk = x0\n",
        "\n",
        "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
        "    \n",
        "    pk = -grad_f(xk)\n",
        "    xk = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
        "    i+=1\n",
        "    \n",
        "  print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
        "  return xk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKXNBar6zlfC",
        "outputId": "b491958b-75af-4e84-fdf7-9255325ce6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem himmelblau: \n",
            "\n",
            "\n",
            "Algorithm output exact gradient: \n",
            "\n",
            " Search terminated after iteration 2816 with result: [-3.77931026 -3.28318599]\n",
            "\n",
            "actual minima: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Last gradient norm: 2.6048138970693457e-07\n",
            "\n",
            "Difference to real solution: 2.5521830106979e-07\n",
            "\n",
            "Algorithm output approximated gradient: \n",
            "\n",
            " Search terminated after iteration 1922 with result: [-3.77931026 -3.283186  ]\n",
            "\n",
            "actual minima: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Last gradient norm: 2.0923775963524984e-07\n",
            "\n",
            "Difference to real solution: 2.62125228242668e-07\n",
            "\n",
            "Problem poly_1: \n",
            "\n",
            "\n",
            "Algorithm output exact gradient: \n",
            "\n",
            " Search terminated after iteration 221 with result: [7.00000006]\n",
            "\n",
            "actual minima: [[3]\n",
            " [5]\n",
            " [7]]\n",
            "\n",
            "Last gradient norm: 4.627252097937028e-07\n",
            "\n",
            "Difference to real solution: 5.784064871505734e-08\n",
            "\n",
            "Algorithm output approximated gradient: \n",
            "\n",
            " Search terminated after iteration 210 with result: [2.99999994]\n",
            "\n",
            "actual minima: [[3]\n",
            " [5]\n",
            " [7]]\n",
            "\n",
            "Last gradient norm: 4.6674944097515907e-07\n",
            "\n",
            "Difference to real solution: 4.000000063843677\n"
          ]
        }
      ],
      "source": [
        "#left this in to show how the problem class works etc.\n",
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output exact gradient: \")\n",
        "x_ = steepest_descent(np.array([0,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
        "print(\"\\nAlgorithm output approximated gradient: \")\n",
        "x_ = steepest_descent(np.array([0,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.poly_1()\n",
        "print(f\"\\nProblem poly_1: \\n\")\n",
        "print(\"\\nAlgorithm output exact gradient: \")\n",
        "x_ = steepest_descent(np.array([1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
        "print(\"\\nAlgorithm output approximated gradient: \")\n",
        "x_ = steepest_descent(np.array([1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMLyr6f7DrBf"
      },
      "source": [
        "alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf. newton_method() implements the line search Newton method solving the equation H @ pk = -grad instead of computing the inverse of H."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "loapUToqBNOO"
      },
      "outputs": [],
      "source": [
        "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
        "\n",
        "  alpha = 0\n",
        "  beta = np.Inf\n",
        "  t = 1\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if f(xk + t * pk) > (f(xk) + c1 * t * (pk @ grad_f(xk))):\n",
        "      beta = t\n",
        "      t = 0.5 * (alpha + beta)\n",
        "    elif (-pk @ grad_f(xk + t * pk)) > (-c2 * pk @ grad_f(xk)):\n",
        "      alpha = t\n",
        "      t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
        "    else:\n",
        "      return t\n",
        "\n",
        "def newton_method(x0, f, grad_f=None, hessian_f=None):\n",
        "  \n",
        "  conv_tol = 1e-8\n",
        "  if grad_f == None:\n",
        "    grad_f = approx_grad(f)\n",
        "    hessian_f = approx_hessian(f)\n",
        "  i = 0\n",
        "  x = x0\n",
        "  \n",
        "  while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
        "    \n",
        "    pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else solve(hessian_f(x), -grad_f(x))\n",
        "    x = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
        "    i += 1\n",
        "  \n",
        "  print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYmj7uuDWmG"
      },
      "source": [
        "For the newton method I first struggled with solving the himmelblau function, but after some tinkering with the starting point, it worked.\n",
        "Also choosing 0s as starting points seems to not work here at all. In the end I reached VERY fast convergence for all problems though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Dqi4AWCAp-",
        "outputId": "20d0671e-cc22-49bb-91c1-50c8049596cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem himmelblau: \n",
            "\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 6 with result: [3. 2.]\n",
            "\n",
            "actual minima: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Last gradient norm: 4.39446412658135e-14\n",
            "\n",
            "Difference to real solution: 1.4043333874306805e-15\n"
          ]
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([2.5,1.5]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rqHHXq9EY3F"
      },
      "source": [
        "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HoO81fsaD-FG"
      },
      "outputs": [],
      "source": [
        "def FR(x0, f, grad_f=None):\n",
        "\n",
        "  conv_tol = 1e-8\n",
        "  if grad_f == None:\n",
        "    grad_f = approx_grad(f)\n",
        "  i = 0\n",
        "  xk = x0\n",
        "  pk = -grad_f(xk)\n",
        "\n",
        "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
        "\n",
        "    xk1 = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
        "    beta = (grad_f(xk1) @ grad_f(xk1)) / (grad_f(xk) @ grad_f(xk))\n",
        "    pk = -grad_f(xk1) + beta * pk\n",
        "    xk = xk1\n",
        "    i += 1\n",
        " \n",
        "  print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
        "  return xk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8yFRBxNEitW",
        "outputId": "eea61628-81e1-4b48-9c5f-be39541c2436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem himmelblau: \n",
            "\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 502 with result: [ 3.58442834 -1.84812652]\n",
            "\n",
            "actual minima: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Last gradient norm: 2.5406398925923756e-07\n",
            "\n",
            "Difference to real solution: 6.237319089753625e-07\n"
          ]
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[3])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhh1aWSrGIRs"
      },
      "source": [
        "SR1() implements the line search quasi newton method utilizing SR1 updating for inverse Hessian approximation and a method for stabilizing that resets H as a multiple of I inspired by the method deployed in https://www.sciencedirect.com/science/article/pii/S0898122111004202?via%3Dihub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gVJ0wT7BFWsF"
      },
      "outputs": [],
      "source": [
        "def SR1(x0 : np.array, f, grad_f=None, r=1e-8):\n",
        "\n",
        "  conv_tol = 1e-8\n",
        "  if grad_f == None:\n",
        "    grad_f = approx_grad(f)\n",
        "  i = 0\n",
        "  H = np.identity(x0.size)\n",
        "  x = x0\n",
        "\n",
        "  while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
        "  \n",
        "    pk = - H @ grad_f(x)\n",
        "    x_1 = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
        "    sk = x_1 - x\n",
        "    yk = grad_f(x_1) - grad_f(x)\n",
        "    rhok = 1 / (yk.T @ sk)\n",
        "    \n",
        "    if ((sk @ yk - yk @ H @ yk) < 0) or (abs(yk @ (sk - H @ yk)) < r * np.linalg.norm(yk) * np.linalg.norm(sk - H @ yk)) or (\n",
        "        norm(H, np.inf) > 1e10):\n",
        "      mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n",
        "      H_1 = mu * np.identity(x0.size)\n",
        "      H = H_1\n",
        "    else:\n",
        "      H_1 = H + np.outer((sk - H @ yk), (sk - H @ yk)) / ((sk - H @ yk) @ yk)\n",
        "      H = H_1\n",
        "    x = x_1\n",
        "    i += 1\n",
        "\n",
        "  print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkZV38tKGrel",
        "outputId": "e5b3ff83-ff39-4eff-b17c-26152a308d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem himmelblau: \n",
            "\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 11 with result: [3. 2.]\n",
            "\n",
            "actual minima: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Last gradient norm: 1.3802086492309621e-07\n",
            "\n",
            "Difference to real solution: 4.5270257313018665e-09\n"
          ]
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rosenbrock [1.2,1.2] starting point**"
      ],
      "metadata": {
        "id": "SSmu1kaRcFF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [1.2,1.2]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([1.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [1.2,1.2]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [1.2,1.2]: \")\n",
        "x_ = SR1(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Skp5VncEqT",
        "outputId": "8fafb866-e85f-4b4a-935d-728c2368f6d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem rosenbrock steepest descent [1.2,1.2]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [1.00225488 1.00455996]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 0.01630683503525153\n",
            "\n",
            "Difference to real solution: 0.005087017651115929\n",
            "\n",
            "Problem rosenbrock steepest descent [1.2,1.2] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [1.00225053 1.00455368]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 0.017387938456607084\n",
            "\n",
            "Difference to real solution: 0.005079460238159602\n",
            "\n",
            "Problem rosenbrock newton method [1.2,1.2]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 8 with result: [1. 1.]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.4360392201267428e-11\n",
            "\n",
            "Difference to real solution: 1.7227828323731315e-13\n",
            "\n",
            "Problem rosenbrock newton method [1.2,1.2] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 6 with result: [0.99999637 0.99999274]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 5.946239652784435e-07\n",
            "\n",
            "Difference to real solution: 8.120916393507349e-06\n",
            "\n",
            "Problem rosenbrock FR [1.2,1.2]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1031 with result: [1.00000028 1.00000055]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.20863559177819e-06\n",
            "\n",
            "Difference to real solution: 6.199506706177205e-07\n",
            "\n",
            "Problem rosenbrock FR [1.2,1.2] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [0.99999851 0.99999701]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 4.297115244228807e-06\n",
            "\n",
            "Difference to real solution: 3.3432608774338093e-06\n",
            "\n",
            "Problem rosenbrock:\n",
            "\n",
            "Algorithm output SR1 [1.2,1.2]: \n",
            "\n",
            " Search terminated after iteration 24 with result: [1. 1.]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.2700270809299206e-08\n",
            "\n",
            "Difference to real solution: 2.5271779361523775e-09\n",
            "\n",
            "Problem rosenbrock SR1 [1.2,1.2] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 24 with result: [0.9999967  0.99999339]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.2690894011094514e-08\n",
            "\n",
            "Difference to real solution: 7.393690821081246e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rosenbrock [-1.2,1] starting point**"
      ],
      "metadata": {
        "id": "Im-apNvwUEsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C0zGoM_s01no",
        "outputId": "6ee66bbe-5959-4d75-a387-6b501cbcf089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem rosenbrock steepest descent [-1.2,1]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [1.65578121 2.73795237]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 3.806006606382066\n",
            "\n",
            "Difference to real solution: 1.8575595378237926\n",
            "\n",
            "Problem rosenbrock steepest descent [-1.2,1] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [1.65585894 2.73768054]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 4.170787005767374\n",
            "\n",
            "Difference to real solution: 1.8573326615477799\n",
            "\n",
            "Problem rosenbrock newton method [-1.2,1]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 21 with result: [1. 1.]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 4.473328123560842e-10\n",
            "\n",
            "Difference to real solution: 1.35118656593187e-10\n",
            "\n",
            "Problem rosenbrock newton method [-1.2,1] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 23 with result: [0.99999669 0.99999338]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 5.404347693928773e-12\n",
            "\n",
            "Difference to real solution: 7.39619433403795e-06\n",
            "\n",
            "Problem rosenbrock FR [-1.2,1]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1401 with result: [0.99999919 0.99999836]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 2.276487604330617e-06\n",
            "\n",
            "Difference to real solution: 1.8289129519062067e-06\n",
            "\n",
            "Problem rosenbrock FR [-1.2,1] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [0.99999782 0.99999564]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 2.8198769777923287e-06\n",
            "\n",
            "Difference to real solution: 4.877405444420461e-06\n",
            "\n",
            "Problem rosenbrock:\n",
            "\n",
            "Algorithm output SR1 [-1.2,1]: \n",
            "\n",
            " Search terminated after iteration 56 with result: [0.99999949 0.99999898]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 7.194182697909295e-07\n",
            "\n",
            "Difference to real solution: 1.1393017526843493e-06\n",
            "\n",
            "Problem rosenbrock SR1 [-1.2,1] approximated gradients:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(\"\\nAlgorithm output: \")\\nx_ = SR1(np.array([-1.2,1]), prob.f)\\nprint(f\"\\nactual minima: {prob.min_x}\")\\ngrad = approx_grad(prob.f)\\nprint(f\"\\nLast gradient norm: {norm(grad(x_))}\")\\nprint(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.2,1]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.2,1]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.2,1] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [-1.2,1]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [-1.2,1] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [-1.2,1]: \")\n",
        "x_ = SR1(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [-1.2,1] approximated gradients:\")\n",
        "#for some reason SR1 with this starting point and approximated gradients seems to be running forever\n",
        "\"\"\"print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rosenbrock [0,1] starting point**"
      ],
      "metadata": {
        "id": "bV80lbwbcjGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,1.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [0.0,1.0]: \")\n",
        "x_ = SR1(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dmjbZ3ZchH-",
        "outputId": "e546f819-c5f1-4d0a-c3fe-c3485a72bc37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem rosenbrock steepest descent [0.0,1.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.34131572 11.17199747]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 5.691070196722854\n",
            "\n",
            "Difference to real solution: 10.437973553036368\n",
            "\n",
            "Problem rosenbrock steepest descent [0.0,1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.3462662  11.19707836]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 5.254216021798363\n",
            "\n",
            "Difference to real solution: 10.46352579469416\n",
            "\n",
            "Problem rosenbrock newton method [0.0,1.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 14 with result: [0.99999997 0.99999994]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.3368392051441223e-06\n",
            "\n",
            "Difference to real solution: 6.766278194441631e-08\n",
            "\n",
            "Problem rosenbrock newton method [0.0,1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 14 with result: [0.99999669 0.99999338]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 3.203369340381293e-09\n",
            "\n",
            "Difference to real solution: 7.396315290542874e-06\n",
            "\n",
            "Problem rosenbrock FR [0.0,1.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1289 with result: [1.000002 1.000004]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.859109609070831e-06\n",
            "\n",
            "Difference to real solution: 4.471812886647933e-06\n",
            "\n",
            "Problem rosenbrock FR [0.0,1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [0.99999363 0.99998725]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 4.3880883583026795e-06\n",
            "\n",
            "Difference to real solution: 1.4249309851679298e-05\n",
            "\n",
            "Problem rosenbrock:\n",
            "\n",
            "Algorithm output SR1 [0.0,1.0]: \n",
            "\n",
            " Search terminated after iteration 36 with result: [0.9999998  0.99999959]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.823289368723782e-07\n",
            "\n",
            "Difference to real solution: 4.5655043845735136e-07\n",
            "\n",
            "Problem rosenbrock SR1 [0.0,1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 36 with result: [0.99999649 0.99999297]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.8602449226346103e-07\n",
            "\n",
            "Difference to real solution: 7.861991136592512e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rosenbrock [-1,0] starting point**"
      ],
      "metadata": {
        "id": "Jiq4MRUJcv2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [-1.0,0.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [-1.0,0.0]: \")\n",
        "x_ = SR1(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlWrpjIIcwBj",
        "outputId": "db8418b0-98ec-43b5-ca11-e30adc28b8ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem rosenbrock steepest descent [-1.0,0.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [2.02427908 4.0962348 ]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 3.252971195764253\n",
            "\n",
            "Difference to real solution: 3.261260123993077\n",
            "\n",
            "Problem rosenbrock steepest descent [-1.0,0.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [2.02709845 4.10769082]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 3.2324567581309087\n",
            "\n",
            "Difference to real solution: 3.2730220662961247\n",
            "\n",
            "Problem rosenbrock newton method [-1.0,0.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 20 with result: [1. 1.]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.7598714924073243e-08\n",
            "\n",
            "Difference to real solution: 1.1806857315577895e-09\n",
            "\n",
            "Problem rosenbrock newton method [-1.0,0.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-0.96181955  0.92986517]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 2.296502735363223\n",
            "\n",
            "Difference to real solution: 1.9630728020944417\n",
            "\n",
            "Problem rosenbrock FR [-1.0,0.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 2491 with result: [1.00000479 1.0000096 ]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 4.468991469697121e-06\n",
            "\n",
            "Difference to real solution: 1.0732133347638919e-05\n",
            "\n",
            "Problem rosenbrock FR [-1.0,0.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [1.00000813 1.00001655]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 0.00010983625151383885\n",
            "\n",
            "Difference to real solution: 1.8445208650430315e-05\n",
            "\n",
            "Problem rosenbrock:\n",
            "\n",
            "Algorithm output SR1 [-1.0,0.0]: \n",
            "\n",
            " Search terminated after iteration 26 with result: [0.99999681 0.9999936 ]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 2.915512641117221e-06\n",
            "\n",
            "Difference to real solution: 7.152197138909946e-06\n",
            "\n",
            "Problem rosenbrock SR1 [-1.0,0.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 26 with result: [0.99999345 0.99998688]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 2.9648997709621905e-06\n",
            "\n",
            "Difference to real solution: 1.4669456825577886e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rosenbrock [0,-1] starting point**"
      ],
      "metadata": {
        "id": "PyH8r8IIc9hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,-1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [0.0,-1.0]: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oInw3wzKc9Xk",
        "outputId": "62e12034-bc92-490a-dc30-efad82307b0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem rosenbrock steepest descent [0.0,-1.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 4.44548393 19.76240537]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 6.752317302747934\n",
            "\n",
            "Difference to real solution: 19.07614255801942\n",
            "\n",
            "Problem rosenbrock steepest descent [0.0,-1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 4.44571041 19.76417378]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 7.189041926562587\n",
            "\n",
            "Difference to real solution: 19.077922787541493\n",
            "\n",
            "Problem rosenbrock newton method [0.0,-1.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 14 with result: [0.99999997 0.99999994]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.1329201043806194e-06\n",
            "\n",
            "Difference to real solution: 6.611754377648865e-08\n",
            "\n",
            "Problem rosenbrock newton method [0.0,-1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 21 with result: [0.99999669 0.99999338]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 7.04489559304088e-11\n",
            "\n",
            "Difference to real solution: 7.3961977068807766e-06\n",
            "\n",
            "Problem rosenbrock FR [0.0,-1.0]:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1544 with result: [0.999999 0.999998]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.8728617384285688e-06\n",
            "\n",
            "Difference to real solution: 2.2389764751719345e-06\n",
            "\n",
            "Problem rosenbrock FR [0.0,-1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 613 with result: [0.99999824 0.99999648]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 1.4451655359864687e-06\n",
            "\n",
            "Difference to real solution: 3.933943098940593e-06\n",
            "\n",
            "Problem rosenbrock:\n",
            "\n",
            "Algorithm output SR1 [0.0,-1.0]: \n",
            "\n",
            " Search terminated after iteration 42 with result: [0.99999999 0.99999998]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 5.336521514825123e-07\n",
            "\n",
            "Difference to real solution: 2.0481947827160615e-08\n",
            "\n",
            "Problem rosenbrock SR1 [0.0,-1.0] approximated gradients:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 42 with result: [0.99999669 0.99999336]\n",
            "\n",
            "actual minima: [1 1]\n",
            "\n",
            "Last gradient norm: 5.986494941989063e-07\n",
            "\n",
            "Difference to real solution: 7.417260466448856e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function 2 [-0.2,1.2] starting point**"
      ],
      "metadata": {
        "id": "Ivj3CII8dacl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-0.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvbNrGZKdaS4",
        "outputId": "a72d29a3-f606-4af3-b418-0a6d484b5149"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem func_2 [-0.2,1.2] steepest descent:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [5.51036353e+00 3.95098441e-04]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 6.665965881415533\n",
            "\n",
            "Difference to real solution: 1.5103635842778416\n",
            "\n",
            "Problem func_2 [-0.2,1.2] steepest descent approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [5.51036372e+00 3.95078555e-04]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 6.665786351193088\n",
            "\n",
            "Difference to real solution: 1.5103637673245058\n",
            "\n",
            "Problem func_2 [-0.2,1.2] newton method:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-0.15716519  0.72412715]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 25.512761601844268\n",
            "\n",
            "Difference to real solution: 4.219760958557142\n",
            "\n",
            "Problem func_2 [-0.2,1.2] newton method approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-0.2101908   0.28577899]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 7.083326282599053\n",
            "\n",
            "Difference to real solution: 0.744507769217765\n",
            "\n",
            "Problem func_2 [-0.2,1.2] FR:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1196 with result: [ 4.00000064e+00 -4.29308378e-10]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 8.401892871224557e-07\n",
            "\n",
            "Difference to real solution: 6.438452483592957e-07\n",
            "\n",
            "Problem func_2 [-0.2,1.2] FR approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-3.92806172e-09  1.00000012e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.353020290722297e-06\n",
            "\n",
            "Difference to real solution: 1.2346020497660857e-07\n",
            "\n",
            "Problem func_2:\n",
            "\n",
            "Algorithm output SR1: \n",
            "\n",
            " Search terminated after iteration 8 with result: [-3.77800315e-10  1.00000000e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.120511705311994e-07\n",
            "\n",
            "Difference to real solution: 8.961362158955249e-10\n",
            "\n",
            "Problem func_2 [-0.2,1.2] SR1 approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 8 with result: [-5.85032335e-09  9.99999997e-01]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.7650755576754819e-06\n",
            "\n",
            "Difference to real solution: 6.7264136442938315e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function 2 [3.8,0.1] starting point**"
      ],
      "metadata": {
        "id": "CkJky8GNdo_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([3.8,0.1]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg-qTessdpI0",
        "outputId": "bd1d28c8-a583-4615-ee7e-b0e526ce2a1b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem func_2 [3.8,0.1] steepest descent:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.91716956e+00 -7.93466501e-06]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.20644969894832185\n",
            "\n",
            "Difference to real solution: 0.08283043802478735\n",
            "\n",
            "Problem func_2 [3.8,0.1] steepest descent approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.91716958e+00 -7.94289992e-06]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.2064868717080603\n",
            "\n",
            "Difference to real solution: 0.08283042262003584\n",
            "\n",
            "Problem func_2 [3.8,0.1] newton method:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [1.45086007 0.06064898]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 33.68962752643022\n",
            "\n",
            "Difference to real solution: 2.54986130706858\n",
            "\n",
            "Problem func_2 [3.8,0.1] newton method approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 6 with result: [ 4.00000002e+00 -5.50653775e-09]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 2.6442365081995605e-05\n",
            "\n",
            "Difference to real solution: 1.742721351625838e-08\n",
            "\n",
            "Problem func_2 [3.8,0.1] FR:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 648 with result: [ 4.00000004e+00 -8.55708028e-10]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 4.027787922450032e-06\n",
            "\n",
            "Difference to real solution: 4.326126245887441e-08\n",
            "\n",
            "Problem func_2 [3.8,0.1] FR approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 798 with result: [-1.35964868e-08  1.00000044e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 4.720095184801884e-06\n",
            "\n",
            "Difference to real solution: 4.355986726864202e-07\n",
            "\n",
            "Problem func_2:\n",
            "\n",
            "Algorithm output SR1: \n",
            "\n",
            " Search terminated after iteration 10 with result: [ 4.0000000e+00 -7.6063742e-14]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 3.711577428236621e-10\n",
            "\n",
            "Difference to real solution: 2.7211097308263647e-12\n",
            "\n",
            "Problem func_2 [3.8,0.1] SR1 approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 10 with result: [ 4.00000002e+00 -5.50695012e-09]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 2.6444367135319202e-05\n",
            "\n",
            "Difference to real solution: 1.7418164052928244e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function 2 [0,0] starting point**"
      ],
      "metadata": {
        "id": "ziFsa2GHdzt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "#This raises linalgerror for singular matrix\n",
        "\"\"\"print(f\"\\nProblem func_2 [0.0,0.0] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\"\"\"\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJG0EmTndz1U",
        "outputId": "2cff35bd-3f79-421a-a726-c86370040edf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem func_2 [0.0,0.0] steepest descent:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.81092217e+00 -2.11855104e-05]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.4800388337462324\n",
            "\n",
            "Difference to real solution: 0.18907783302949793\n",
            "\n",
            "Problem func_2 [0.0,0.0] steepest descent approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.81092213e+00 -2.11761444e-05]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.4799988174375614\n",
            "\n",
            "Difference to real solution: 0.18907786677445806\n",
            "\n",
            "Problem func_2 [0.0,0.0] newton method approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [0.27246778 0.13623389]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 3.3318726179246263\n",
            "\n",
            "Difference to real solution: 0.9057210293803214\n",
            "\n",
            "Problem func_2 [0.0,0.0] FR:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 525 with result: [-2.6240626e-10  1.0000000e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 7.731063295839679e-08\n",
            "\n",
            "Difference to real solution: 9.605712033403448e-10\n",
            "\n",
            "Problem func_2 [0.0,0.0] FR approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-1.33466244e-08  1.00000013e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 3.884655188044319e-06\n",
            "\n",
            "Difference to real solution: 1.2677254632733648e-07\n",
            "\n",
            "Problem func_2:\n",
            "\n",
            "Algorithm output SR1: \n",
            "\n",
            " Search terminated after iteration 17 with result: [-1.62394148e-14  1.00000000e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 4.867910126477023e-12\n",
            "\n",
            "Difference to real solution: 1.7310721210928362e-14\n",
            "\n",
            "Problem func_2 [0.0,0.0] SR1 approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 17 with result: [-5.47251623e-09  9.99999996e-01]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.6533404375869652e-06\n",
            "\n",
            "Difference to real solution: 6.857170204415158e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function 2 [-1,0] starting point**"
      ],
      "metadata": {
        "id": "0DQ1Zg6ZdK6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m41Z1lwTrSe",
        "outputId": "6fa39d8a-7071-40aa-da33-d92283893947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem func_2 [-1,0] steepest descent:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 626 with result: [-3.37092675e-10  1.00000000e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.0130333293170866e-07\n",
            "\n",
            "Difference to real solution: 3.3710057616776203e-10\n",
            "\n",
            "Problem func_2 [-1,0] steepest descent approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 462 with result: [-5.61268048e-09  9.99999996e-01]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.6954740107879791e-06\n",
            "\n",
            "Difference to real solution: 6.975309930459983e-09\n",
            "\n",
            "Problem func_2 [-1,0] newton method:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1 with result: [4. 0.]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.0\n",
            "\n",
            "Difference to real solution: 0.0\n",
            "\n",
            "Problem func_2 [-1,0] newton method approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-1.039925    0.01466632]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 5.798837117957822\n",
            "\n",
            "Difference to real solution: 1.4325943095740383\n",
            "\n",
            "Problem func_2 [-1,0] FR:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 786 with result: [-2.98617162e-10  1.00000001e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 9.0585639243895e-08\n",
            "\n",
            "Difference to real solution: 5.814496623160658e-09\n",
            "\n",
            "Problem func_2 [-1,0] FR approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [-4.19125937e-09  1.00000012e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.4077242017305186e-06\n",
            "\n",
            "Difference to real solution: 1.2330707147206263e-07\n",
            "\n",
            "Problem func_2:\n",
            "\n",
            "Algorithm output SR1: \n",
            "\n",
            " Search terminated after iteration 22 with result: [-4.15379491e-13  1.00000000e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.313655814487179e-10\n",
            "\n",
            "Difference to real solution: 1.0010620794716596e-11\n",
            "\n",
            "Problem func_2 [-1,0] SR1 approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 21 with result: [-5.47250064e-09  9.99999996e-01]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.6533357676502962e-06\n",
            "\n",
            "Difference to real solution: 6.857161376817747e-09\n"
          ]
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-1,0] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-1,0] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1,0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-1,0] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([-1,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function 2 [0,-1] starting point**"
      ],
      "metadata": {
        "id": "Sav7POtNd8v-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4chOrUdBhrVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d40e7a9c-923f-4330-86cd-7d2e4f68c3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem func_2 [0.0,-1.0] steepest descent:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.62067233e+00 -4.16454870e-05]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.9420773236315432\n",
            "\n",
            "Difference to real solution: 0.37932767135725237\n",
            "\n",
            "Problem func_2 [0.0,-1.0] steepest descent approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 3.62204949e+00 -3.71714819e-05]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.9220844961370029\n",
            "\n",
            "Difference to real solution: 0.3779505077343804\n",
            "\n",
            "Problem func_2 [0.0,-1.0] newton method:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 1 with result: [0. 1.]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 0.0\n",
            "\n",
            "Difference to real solution: 0.0\n",
            "\n",
            "Problem func_2 [0.0,-1.0] FR:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 478 with result: [5.61731937e-10 9.99999996e-01]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.6393919798806885e-07\n",
            "\n",
            "Difference to real solution: 3.6662287733588423e-09\n",
            "\n",
            "Problem func_2 [0.0,-1.0] FR approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 5001 with result: [ 4.00000076e+00 -1.67043511e-09]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 6.529872416520798e-06\n",
            "\n",
            "Difference to real solution: 7.561741813515493e-07\n",
            "\n",
            "Problem func_2:\n",
            "\n",
            "Algorithm output SR1: \n",
            "\n",
            " Search terminated after iteration 19 with result: [-1.33427622e-10  1.00000000e+00]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 4.000757977291459e-08\n",
            "\n",
            "Difference to real solution: 1.4041828298924607e-10\n",
            "\n",
            "Problem func_2 [0.0,-1.0] SR1 approximate gradient:\n",
            "\n",
            "Algorithm output: \n",
            "\n",
            " Search terminated after iteration 19 with result: [-5.60624347e-09  9.99999996e-01]\n",
            "\n",
            "actual minima: [[0 1]\n",
            " [4 0]]\n",
            "\n",
            "Last gradient norm: 1.693424070931609e-06\n",
            "\n",
            "Difference to real solution: 6.939055604419462e-09\n"
          ]
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "#this raises linalgerror singular matrix\n",
        "\"\"\"print(f\"\\nProblem func_2 [0.0,-1.0] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\"\"\"\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TOh95c0NmZ2b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project1_phase2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}