{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZ8wEbZnBnQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Project 1 Phase 2**<br>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Michael</td>\n",
    "    <td style = \"text-align: left\">Weikl</td>\n",
    "    <td style = \"text-align: left\">1154652</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Uros</td>\n",
    "    <td style = \"text-align: left\">Zivanovic</td>\n",
    "    <td style = \"text-align: left\">12032271</td>\n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Table of contents\n",
    "* [Problemdefinition](#Problemdefinition)\n",
    "* [Basic functions](#Basic-functions)\n",
    "    * [Stopping Criterion](#Stopping-Criterion)\n",
    "    * [Backtracking](#Backtracking)\n",
    "    * [Wolfe Condition](#Wolfe-Condition)\n",
    "    * [Derivation (Taylor-Theorem)](Derivation-(Taylor-Theorem))\n",
    "* [Implementation of the algorithms](#Implementation-of-the-algorithms)\n",
    "    * [Steepest descent](#Steepest-descent)\n",
    "    * [Newthon method](#Newthon-method)\n",
    "    * [Fetcher Reeves](#Fetcher-Reeves)\n",
    "    * [SR1](#SR1)\n",
    "* [Executions](#Executions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KwrOzH2PmHIH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqP_WueRyEc-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic Problem class to set up problems for the algorithms to be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eNaoAtoPmPM-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "    def f(self, x):\n",
    "        raise NotImplementedError(\"f() is not implemented for this Problem\")\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        raise NotImplementedError(\"grad_f() is not implemented for this Problem\")\n",
    "            \n",
    "    def hessian(self, x):\n",
    "        raise NotImplementedError(\"hessian() is not implemented for this Problem\")      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " class PolynomialProblem(Problem):\n",
    "        @property\n",
    "        def actual_minima(self):\n",
    "            raise NotImplementedError(\"actual_minima() is not implemented for this Problem\")      \n",
    "\n",
    "\n",
    "############################################################\n",
    "#                Polynomial definitions\n",
    "############################################################\n",
    "class Himmelblau(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                         4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian(self, x):\n",
    "        return np.array([[12*x[0]**2 + 4*x[1] - 42, \n",
    "                          4*(x[1] + x[0])],\n",
    "                         [4*(x[1] + x[0]),\n",
    "                          12*x[1]**2 + 4*x[0] - 26]])\n",
    "    \n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([[3,2], \n",
    "                         [-2.805118, 3.131312], \n",
    "                         [-3.779310, -3.283186], \n",
    "                         [3.584428, -1.848126]])\n",
    "\n",
    "    \n",
    "class Polynomial1(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian(self, x):\n",
    "        return 3 * x**2 - 30 * x + 71\n",
    "    \n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([[3],[5],[7]])\n",
    "\n",
    "    \n",
    "class Rosenbrock(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(self, x):\n",
    "        return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "                         200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian(self, x):\n",
    "        return np.array([[-400*(x[1] - 3*x[0]**2) + 2, \n",
    "                          -400*x[0]],\n",
    "                         [-400*x[0], \n",
    "                          200]])\n",
    "    \n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([1,1])\n",
    "\n",
    "\n",
    "#this is the function below rosenbrock defined in the project description\n",
    "class Function2(PolynomialProblem):\n",
    "    def f(self, x):\n",
    "        return 150*(x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "    \n",
    "    def grad_f(self, x):\n",
    "        return np.array([x[0] * (300 * x[1]**2 + 0.5) + 2 * x[1] - 2, 300*x[0]**2 * x[1] + 2 * x[0] + 8 * x[1] - 8])\n",
    "\n",
    "    def hessian(self, x):\n",
    "        return np.array([[300 * x[1]**2 + 0.5, 600 * x[0]*x[1] + 2],[600 * x[0]*x[1] + 2, 300 * x[0]**2 + 8]])\n",
    "\n",
    "    #self calculated minima\n",
    "    @property\n",
    "    def actual_minima(self):\n",
    "        return np.array([[0,1],[4,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for squared error problems\n",
    "def normal_distributed_data_points(target_function, q: float, num_points: int):\n",
    "    # bring normal distributed value from range [0, 1] to  [-q, q]\n",
    "    return distributed_data_points(target_function=target_function, q=q, num_points=num_points,\n",
    "                                   distribution=lambda r: (np.random.normal(0, scale=2 * r) - r))\n",
    "\n",
    "\n",
    "def uniform_distributed_data_points(target_function, q: float, num_points: int):\n",
    "    return distributed_data_points(target_function=target_function, q=q, num_points=num_points,\n",
    "                                   distribution=lambda r: np.random.uniform(low=-r, high=r))\n",
    "\n",
    "\n",
    "def distributed_data_points(target_function, q: float, num_points: int, distribution):\n",
    "    result_list = []\n",
    "\n",
    "    for _ in range(num_points):\n",
    "        val = distribution(abs(q))\n",
    "        result_list.append((val, target_function(val)))\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SquaredErrorProblem(Problem):\n",
    "    def __init__(self, target_function,\n",
    "                 q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        if distribution == 'uniform':\n",
    "            self.data = uniform_distributed_data_points(target_function=target_function, q=q, num_points=num_points)\n",
    "        elif distribution == 'normal':\n",
    "            self.data = normal_distributed_data_points(target_function=target_function, q=q, num_points=num_points)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Functionality not implemented for ditribution type '{distribution}'. Use 'uniform' or 'normal'.\")\n",
    "\n",
    "        self.coefficients = []\n",
    "        for j in range(num_points):\n",
    "            a_j = self.data[j][0]\n",
    "            c_j = np.reshape([a_j ** i for i in range(degree + 1)], (degree + 1, 1))\n",
    "            self.coefficients.append(c_j)\n",
    "\n",
    "        self.target_function = target_function\n",
    "\n",
    "    def __residual(self, x, j):\n",
    "        a_j = self.data[j][0]\n",
    "        b_j = self.data[j][1]\n",
    "        c_j = self.coefficients[j]\n",
    "\n",
    "        return c_j.T @ x - b_j\n",
    "\n",
    "    def f(self, x):\n",
    "        return 1 / 2 * np.sum([self.__residual(x, j) ** 2 for j in range(len(self.data))])\n",
    "\n",
    "    def grad_f(self, x):\n",
    "        return np.sum([self.__residual(x, j) * self.coefficients[j] for j in range(len(self.data))], axis=0)\n",
    "\n",
    "    def hessian(self, x):\n",
    "        return np.sum([self.coefficients[j] @ self.coefficients[j].T for j in range(len(self.data))], axis=0)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#              Squared Error Problem Definition\n",
    "############################################################\n",
    "\n",
    "class SinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: np.sin(x), q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class CosinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: np.cos(x), q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class SinusMinusCosinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: np.sin(x) - np.cos(x), q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class CubePlusSquareSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: x ** 3 + x ** 2, q, num_points, degree, random_seed, distribution)\n",
    "\n",
    "\n",
    "class SinusPlusCosinusSQEP(SquaredErrorProblem):\n",
    "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
    "                 random_seed: int = 1154652, distribution='uniform'):\n",
    "        super().__init__(lambda x: 3 * np.sin(x) + np.cos(x), q, num_points, degree, random_seed, distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stopping Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This implements a stopping criterion for when the gradient of f at xk is small relative to the gradient of f at x0\n",
    "Also a maxmimum number of iterations is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, current_iteration, tol: float = 1e-8, max_iter: int = 1000):\n",
    "    if current_iteration > max_iter:\n",
    "        return True\n",
    "    elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Backtracking linesearch to find suitable step length as described in the book. steepest_descent() implements the line search steepest descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
    "    alpha = alpha0\n",
    "    f_xk = f(xk)\n",
    "    grad_f_xk = grad_f(xk)\n",
    "\n",
    "    while f(xk + alpha * pk) > (f_xk + c * alpha * grad_f_xk.T @ pk):\n",
    "        alpha *= rho\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wolfe Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf. newton_method() implements the line search Newton method solving the equation H @ pk = -grad instead of computing the inverse of H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
    "    alpha = 0\n",
    "    beta = np.Inf\n",
    "    t = 1\n",
    "\n",
    "    while True:\n",
    "        if f(xk + t * pk) > (f(xk) + c1 * t * (pk.T @ grad_f(xk))):\n",
    "            beta = t\n",
    "            t = 0.5 * (alpha + beta)\n",
    "        elif (-pk.T @ grad_f(xk + t * pk)) > (-c2 * pk.T @ grad_f(xk)):\n",
    "            alpha = t\n",
    "            t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
    "        else:\n",
    "            return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Derivation (Taylor Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This implements the forward difference approach from the book for gradient and hessian calculation without explicit function knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "    arr = np.zeros(size)\n",
    "    arr[index] = 1.0\n",
    "    return arr\n",
    "\n",
    "\n",
    "# these return the function that computes the gradient, which can then in turn be called to compute the gradient\n",
    "def approx_grad(f, e=1.1e-8):\n",
    "    def grad_f(x):\n",
    "        if x.size == 1:\n",
    "            return (f(x + e) - f(x)) / e\n",
    "        return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "\n",
    "    return grad_f\n",
    "\n",
    "\n",
    "def approx_hessian(f, e=1.1e-8):\n",
    "    def hessian_f(x):\n",
    "        if x.size == 1:\n",
    "            return (f(x + 2 * e) - 2 * f(x + e) + f(x)) / e ** 2\n",
    "        return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "            x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "            x)) / e ** 2 for j in range(x.size)] for i in range(x.size)])\n",
    "\n",
    "    return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implementation of the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def steepest_descent(x0, f, grad_f=None, initial_alpha: float = 1e-3, tolerance: float = 1e-8):\n",
    "    if grad_f == None:\n",
    "        grad_f = approx_grad(f)\n",
    "\n",
    "    i = 1\n",
    "    xk = x0\n",
    "    search_alpha, alpha, pk = None, None, None\n",
    "\n",
    "    while not stop_crit(grad_f, xk, x0, i, tol=tolerance):\n",
    "\n",
    "        if search_alpha is None:\n",
    "            search_alpha = initial_alpha\n",
    "            pk = -grad_f(xk)\n",
    "        else:\n",
    "            new_pk = -grad_f(xk)\n",
    "            search_alpha = alpha * (-pk.T @ pk) / (-new_pk.T @ new_pk)\n",
    "            pk = new_pk\n",
    "\n",
    "        alpha = backtracking_alpha(f, grad_f, xk, pk)\n",
    "        xk = xk + alpha * pk\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return xk, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def newton_method(x0, f, grad_f=None, hessian_f=None, tolerance=1e-8):\n",
    "    if grad_f == None:\n",
    "        grad_f = approx_grad(f)\n",
    "        hessian_f = approx_hessian(f)\n",
    "    i = 1\n",
    "    x = x0\n",
    "\n",
    "    while not stop_crit(grad_f, x, x0, i, tol=tolerance):\n",
    "        pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else solve(hessian_f(x), -grad_f(x))\n",
    "        x = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "        i += 1\n",
    "\n",
    "    #print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fetcher Reeves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def FR(x0, f, grad_f=None):\n",
    "\n",
    "    conv_tol = 1e-8\n",
    "    if grad_f == None:\n",
    "        grad_f = approx_grad(f)\n",
    "    i = 0\n",
    "    xk = x0\n",
    "    pk = -grad_f(xk)\n",
    "\n",
    "    while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
    "\n",
    "        xk1 = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
    "        beta = (grad_f(xk1) @ grad_f(xk1)) / (grad_f(xk) @ grad_f(xk))\n",
    "        pk = -grad_f(xk1) + beta * pk\n",
    "        xk = xk1\n",
    "        i += 1\n",
    " \n",
    "    #print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def approx_hessian_bfgs(f_grad, B_k, x_1, x):\n",
    "    \"\"\"\n",
    "    Approximate Hessian based on first derivitive.\n",
    "    :param f_grad: first derivitive of f\n",
    "    :param B_k: current hessian\n",
    "    :param x_1: x_{k+1}\n",
    "    :param x: x_{k}\n",
    "    :return: approximated hessian\n",
    "    \"\"\"\n",
    "\n",
    "    s_k = x_1 - x\n",
    "    y_k = f_grad(x_1) - f_grad(x)\n",
    "\n",
    "    return B_k - (B_k * s_k * np.transpose(s_k) * B_k) / \\\n",
    "           (np.transpose(s_k) * B_k * s_k) + \\\n",
    "           (y_k * np.transpose(y_k)) / (np.transpose(y_k) * s_k)\n",
    "\n",
    "\n",
    "def BFGS(x0, f):\n",
    "    tolerance = 1e-8\n",
    "    H = np.eye(x0.shape[0])\n",
    "    x = x0\n",
    "\n",
    "    f_grad = approx_grad(f)\n",
    "\n",
    "    for _ in range(1000):\n",
    "\n",
    "        f_x = f_grad(x)\n",
    "\n",
    "        if np.all(np.abs(H)) < tolerance:\n",
    "            return x\n",
    "\n",
    "        x_old = x\n",
    "        x = x - f_x / H\n",
    "        H = approx_hessian_bfgs(f_x, H, f_grad(x), x_old)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fletcher Reeves"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SR1() implements the line search quasi newton method utilizing SR1 updating for inverse Hessian approximation and a method for stabilizing that resets H as a multiple of I inspired by the method deployed in https://www.sciencedirect.com/science/article/pii/S0898122111004202?via%3Dihub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SR1(x0 : np.array, f, grad_f, r=1e-8):\n",
    "\n",
    "    conv_tol = 1e-8\n",
    "    i = 0\n",
    "    H = np.identity(x0.size)\n",
    "    x = x0\n",
    "\n",
    "    while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
    "  \n",
    "        pk = - H @ grad_f(x)\n",
    "        x_1 = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "        sk = x_1 - x\n",
    "        yk = grad_f(x_1) - grad_f(x)\n",
    "        rhok = 1 / (yk.T @ sk)\n",
    "    \n",
    "        if ((sk @ yk - yk @ H @ yk) < 0) or (abs(yk @ (sk - H @ yk)) < r * np.linalg.norm(yk) * np.linalg.norm(sk - H @ yk)) or (\n",
    "            norm(H, np.inf) > 1e10):\n",
    "            mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n",
    "            H_1 = mu * np.identity(x0.size)\n",
    "            H = H_1\n",
    "        else:\n",
    "            H_1 = H + np.outer((sk - H @ yk), (sk - H @ yk)) / ((sk - H @ yk) @ yk)\n",
    "            H = H_1\n",
    "        \n",
    "        x = x_1\n",
    "        i += 1\n",
    "\n",
    "    #print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define list with all possible algorithms for the tests\n",
    "algorithms = {'Steepest descent': steepest_descent,\n",
    "              'Newthon method': newton_method,\n",
    "              'Fetcher Reeves': FR,\n",
    "              'SR1': SR1,\n",
    "              'BFGS': BFGS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Polynomial Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKXNBar6zlfC",
    "outputId": "b491958b-75af-4e84-fdf7-9255325ce6e5",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "Problem Himmelblau:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 1001 with result: [-3.77938833 -3.28313757]\n",
      "Last gradient norm: 0.012291634078248615\n",
      "Difference to real solutions:\n",
      "\tDifference to point '[3. 2.]' is 8.594861759349271\n",
      "\tDifference to point '[-2.805118  3.131312]' is 6.4880171015771\n",
      "\tDifference to point '[-3.77931  -3.283186]' is 9.209415959377343e-05\n",
      "\tDifference to point '[ 3.584428 -1.848126]' is 7.502336244619459\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Newthon method':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 1001 with result: [-3.49888468e-16 -8.88178420e-16]\n",
      "Last gradient norm: 26.076809620810565\n",
      "Difference to real solutions:\n",
      "\tDifference to point '[3. 2.]' is 3.6055512754639905\n",
      "\tDifference to point '[-2.805118  3.131312]' is 4.20402210213838\n",
      "\tDifference to point '[-3.77931  -3.283186]' is 5.006245537995115\n",
      "\tDifference to point '[ 3.584428 -1.848126]' is 4.03282702320097\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Fetcher Reeves':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration -1.8481265243408067 with result: 3.5844283377990713\n",
      "Algorithm 'Fetcher Reeves' failed to find a solution\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'SR1':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 2.0000000037265804 with result: 2.999999997429677\n",
      "Algorithm 'SR1' failed to find a solution\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'BFGS':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 0 with result: 0\n",
      "Algorithm 'BFGS' failed to find a solution\n",
      "**********************************************\n",
      "Problem Polynomial1:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 222 with result: [7.00000006]\n",
      "Last gradient norm: 4.627252097937028e-07\n",
      "Difference to real solutions:\n",
      "\tDifference to point '[3]' is 4.000000057840649\n",
      "\tDifference to point '[5]' is 2.0000000578406487\n",
      "\tDifference to point '[7]' is 5.784064871505734e-08\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Newthon method':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 7 with result: [3.]\n",
      "Last gradient norm: 3.6624925314880115e-11\n",
      "Difference to real solutions:\n",
      "\tDifference to point '[3]' is 4.5781156643442955e-12\n",
      "\tDifference to point '[5]' is 2.000000000004578\n",
      "\tDifference to point '[7]' is 4.000000000004578\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Fetcher Reeves':\n",
      "------------------------------------------------\n",
      "Algorithm 'Fetcher Reeves' failed to find a solution\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'SR1':\n",
      "------------------------------------------------\n",
      "Algorithm 'SR1' failed to find a solution\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'BFGS':\n",
      "------------------------------------------------\n",
      "Algorithm 'BFGS' failed to find a solution\n"
     ]
    }
   ],
   "source": [
    "polynomial_problems = {Himmelblau(): [0,0], \n",
    "                       Polynomial1(): [1]   }\n",
    "\n",
    "for prob, start_value in polynomial_problems.items():\n",
    "    \n",
    "    print(\"**********************************************\")\n",
    "    print(f\"Problem {type(prob).__name__}:\")\n",
    "    print(\"**********************************************\")\n",
    "    for algorithm_name, algorithm in algorithms.items():\n",
    "        try:\n",
    "            print(\"------------------------------------------------\")\n",
    "            print(f\"\\tAlgorithm '{algorithm_name}':\")\n",
    "            print(\"------------------------------------------------\")\n",
    "            parameters = {'x0': np.array(start_value),\n",
    "                          'f': prob.f,\n",
    "                          'grad_f': prob.grad_f,\n",
    "                          'hessian_f': prob.hessian}\n",
    "\n",
    "            # clean up parameters for generic method call\n",
    "            possible_parameters = algorithm.__code__.co_varnames\n",
    "            for p in list(parameters):\n",
    "                if p not in possible_parameters:\n",
    "                    parameters.pop(p)\n",
    "\n",
    "            x, iterations = algorithm(**parameters)\n",
    "\n",
    "            print(f\"Search terminated after iteration {iterations} with result: {x}\")\n",
    "            print(f\"Last gradient norm: {norm(prob.grad_f(x))}\")\n",
    "            print(\"Difference to real solutions:\")\n",
    "            for am in prob.actual_minima:\n",
    "                print(f\"\\tDifference to point '{am}' is {norm(x - am)}\")\n",
    "        \n",
    "        except Exception:\n",
    "            print(f\"Algorithm '{algorithm_name}' failed to find a solution\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# prob = Problem()\n",
    "# prob.poly_1()\n",
    "# print(f\"\\nProblem poly_1: \\n\")\n",
    "# print(\"\\nAlgorithm output exact gradient: \")\n",
    "# x_ = steepest_descent(np.array([1]), prob.f, prob.grad_f)\n",
    "# print(f\"\\nactual minima: {prob.min_x}\")\n",
    "# print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "# print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
    "# print(\"\\nAlgorithm output approximated gradient: \")\n",
    "# x_ = steepest_descent(np.array([1]), prob.f)\n",
    "# print(f\"\\nactual minima: {prob.min_x}\")\n",
    "# grad = approx_grad(prob.f)\n",
    "# print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "# print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Squared Error Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************\n",
      "Problem SinusSQEP:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 1001 with result: [[-0.00152856]\n",
      " [ 0.81234808]\n",
      " [ 0.01702479]\n",
      " [ 0.03512928]\n",
      " [-0.00662312]\n",
      " [-0.03741755]]\n",
      "Last gradient norm: 5.541382771875275\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "\tAlgorithm 'Newthon method':\n",
      "------------------------------------------------\n",
      "Search terminated after iteration 2 with result: [[ 5.05096262e-05]\n",
      " [ 9.99149991e-01]\n",
      " [-9.65727120e-05]\n",
      " [-1.64589694e-01]\n",
      " [ 3.01540878e-05]\n",
      " [ 7.14423682e-03]]\n",
      "Last gradient norm: 1.427703478112118e-13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "\tAlgorithm 'Fetcher Reeves':\n",
      "------------------------------------------------\n",
      "Algorithm 'Fetcher Reeves' failed to find a solution.\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'SR1':\n",
      "------------------------------------------------\n",
      "Algorithm 'SR1' failed to find a solution.\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'BFGS':\n",
      "------------------------------------------------\n",
      "Algorithm 'BFGS' failed to find a solution.\n",
      "**********************************************\n",
      "Problem CosinusSQEP:\n",
      "**********************************************\n",
      "------------------------------------------------\n",
      "\tAlgorithm 'Steepest descent':\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sqep_problems = {SinusSQEP: {'q': 2, 'num_points': 100, 'degree': 5},\n",
    "                 CosinusSQEP: {'q': 2, 'num_points': 100, 'degree': 5}}\n",
    "\n",
    "for prob_class, prob_params in sqep_problems.items():\n",
    "\n",
    "    prob = prob_class(**prob_params)\n",
    "\n",
    "    prob_num_points = prob_params['num_points']\n",
    "    prob_q = prob_params['q']\n",
    "    prob_degree = prob_params['degree']\n",
    "\n",
    "    x_plot = [i / 10 for i in range(-prob_q * 10, prob_q * 10 + 1)]\n",
    "    y_true_plot = [prob.target_function(x) for x in x_plot]\n",
    "\n",
    "    print(\"**********************************************\")\n",
    "    print(f\"Problem {type(prob).__name__}:\")\n",
    "    print(\"**********************************************\")\n",
    "    for algorithm_name, algorithm in algorithms.items():\n",
    "        try:\n",
    "            print(\"------------------------------------------------\")\n",
    "            print(f\"\\tAlgorithm '{algorithm_name}':\")\n",
    "            print(\"------------------------------------------------\")\n",
    "\n",
    "            parameters = {'x0': np.reshape([0 for i in range(prob_degree + 1)], (prob_degree + 1, 1)),\n",
    "                          'f': prob.f,\n",
    "                          'grad_f': prob.grad_f,\n",
    "                          'hessian_f': prob.hessian}\n",
    "\n",
    "            # clean up parameters for generic method call\n",
    "            possible_parameters = algorithm.__code__.co_varnames\n",
    "            for p in list(parameters):\n",
    "                if p not in possible_parameters:\n",
    "                    parameters.pop(p)\n",
    "\n",
    "            coef, iterations = algorithm(**parameters)\n",
    "            print(f\"Search terminated after iteration {iterations} with result: {coef}\")\n",
    "            print(f\"Last gradient norm: {norm(prob.grad_f(coef))}\")\n",
    "\n",
    "            approximation_y = []\n",
    "            for i in range(len(x_plot)):\n",
    "                approximation_y.append(0)\n",
    "                for d in range(prob_degree):\n",
    "                    approximation_y[i] += coef[d] * x_plot[i] ** d\n",
    "\n",
    "            plt.plot(x_plot, y_true_plot, label=\"true function\")\n",
    "            plt.plot(x_plot, approximation_y, '-.', label=\"approximation\")\n",
    "            plt.title(f'{algorithm_name}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        except Exception:\n",
    "            print(f\"Algorithm '{algorithm_name}' failed to find a solution.\")\n",
    "\n",
    "# def draw_graphs(degree : int = 5, ):\n",
    "#     x = np.reshape([0 for i in range(degree+1)], (degree+1, 1))\n",
    "\n",
    "#     fig, ax = plt.subplots(4, figsize=(10, 15))\n",
    "\n",
    "#     x_multipliers, steps = newton_descent(x, 1000)\n",
    "#     xs = [i/10 for i in range(-q*10, q*10 + 1)]\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[0].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[0].plot(xs, y_approx,'-.', label=\"approximation\")\n",
    "#     ax[0].set(title=f'Newton method after {steps} steps')\n",
    "#     ax[0].legend()\n",
    "\n",
    "#     x_multipliers, steps = conjugate_gradient(x, 15000)\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[3].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[3].plot(xs, y_approx, label=\"approximation\")\n",
    "#     ax[3].set(title=f'Conjugate gradient method after {steps} steps')\n",
    "#     ax[3].legend()\n",
    "\n",
    "#     x_multipliers, steps = steepest_descent(x, 15000)\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[1].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[1].plot(xs, y_approx, label=\"approximation\")\n",
    "#     ax[1].set(title=f'Steepest descent after {steps} steps')\n",
    "#     ax[1].legend()\n",
    "    \n",
    "#     # converges extremely slowly\n",
    "#     \"\"\"x_multipliers, steps = SR1(x, 15000)\n",
    "\n",
    "#     y_true = [fun(x) for x in xs]\n",
    "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
    "\n",
    "#     ax[2].plot(xs, y_true, label=\"true function\")\n",
    "#     ax[2].plot(xs, y_approx, label=\"approximation\")\n",
    "#     ax[2].set(title=f'SR1 after {steps} steps')\n",
    "#     ax[2].legend()\"\"\"\n",
    "\n",
    "#     fig.tight_layout()\n",
    "    \n",
    "# draw_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Old Code #####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRYmj7uuDWmG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the newton method I first struggled with solving the himmelblau function, but after some tinkering with the starting point, it worked.\n",
    "Also choosing 0s as starting points seems to not work here at all. In the end I reached VERY fast convergence for all problems though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9Dqi4AWCAp-",
    "outputId": "20d0671e-cc22-49bb-91c1-50c8049596cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([2.5,1.5]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rqHHXq9EY3F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8yFRBxNEitW",
    "outputId": "eea61628-81e1-4b48-9c5f-be39541c2436",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkZV38tKGrel",
    "outputId": "e5b3ff83-ff39-4eff-b17c-26152a308d90",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSmu1kaRcFF1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Rosenbrock [1.2,1.2] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1Skp5VncEqT",
    "outputId": "8fafb866-e85f-4b4a-935d-728c2368f6d8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([1.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [1.2,1.2]: \")\n",
    "x_ = SR1(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im-apNvwUEsz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Rosenbrock [-1.2,1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C0zGoM_s01no",
    "outputId": "6ee66bbe-5959-4d75-a387-6b501cbcf089",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.2,1]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [-1.2,1]: \")\n",
    "x_ = SR1(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [-1.2,1] approximated gradients:\")\n",
    "#for some reason SR1 with this starting point and approximated gradients seems to be running forever\n",
    "\"\"\"print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV80lbwbcjGM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Rosenbrock [0,1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dmjbZ3ZchH-",
    "outputId": "e546f819-c5f1-4d0a-c3fe-c3485a72bc37",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [0.0,1.0]: \")\n",
    "x_ = SR1(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jiq4MRUJcv2d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Rosenbrock [-1,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlWrpjIIcwBj",
    "outputId": "db8418b0-98ec-43b5-ca11-e30adc28b8ea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [-1.0,0.0]: \")\n",
    "x_ = SR1(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyH8r8IIc9hd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Rosenbrock [0,-1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oInw3wzKc9Xk",
    "outputId": "62e12034-bc92-490a-dc30-efad82307b0f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [0.0,-1.0]: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivj3CII8dacl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Function 2 [-0.2,1.2] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvbNrGZKdaS4",
    "outputId": "a72d29a3-f606-4af3-b418-0a6d484b5149",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-0.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkJky8GNdo_N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Function 2 [3.8,0.1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg-qTessdpI0",
    "outputId": "bd1d28c8-a583-4615-ee7e-b0e526ce2a1b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([3.8,0.1]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziFsa2GHdzt8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Function 2 [0,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJG0EmTndz1U",
    "outputId": "2cff35bd-3f79-421a-a726-c86370040edf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "#This raises linalgerror for singular matrix\n",
    "\"\"\"print(f\"\\nProblem func_2 [0.0,0.0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\"\"\"\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQ1Zg6ZdK6F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Function 2 [-1,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-m41Z1lwTrSe",
    "outputId": "6fa39d8a-7071-40aa-da33-d92283893947",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1,0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sav7POtNd8v-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Function 2 [0,-1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4chOrUdBhrVb",
    "outputId": "d40e7a9c-923f-4330-86cd-7d2e4f68c3ee",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "#this raises linalgerror singular matrix\n",
    "\"\"\"print(f\"\\nProblem func_2 [0.0,-1.0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\"\"\"\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOh95c0NmZ2b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project1_phase2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}