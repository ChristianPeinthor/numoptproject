{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZ8wEbZnBnQ"
   },
   "source": [
    "**Project 1 Phase 2**<br>\n",
    "<br>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Michael</td>\n",
    "    <td style = \"text-align: left\">Weikl</td>\n",
    "    <td style = \"text-align: left\">1154652</td>\n",
    "  </tr>\n",
    "\n",
    "  \n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "* [Problemdefinition](#Problemdefinition)\n",
    "* [Basic functions](#Basic-functions)\n",
    "    * [Stopping Criterion](#Stopping-Criterion)\n",
    "    * [Backtracking](#Backtracking)\n",
    "    * [Wolfe Condition](#Wolfe-Condition)\n",
    "    * [Derivation (Taylor-Theorem)](Derivation-(Taylor-Theorem))\n",
    "* [Implementation of the algorithms](#Implementation-of-the-algorithms)\n",
    "    * [Steepest descent](#Steepest-descent)\n",
    "    * [Newthon method](#Newthon-method)\n",
    "    * [Fetcher Reeves](#Fetcher-Reeves)\n",
    "    * [SR1](#SR1)\n",
    "* [Executions](#Executions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KwrOzH2PmHIH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqP_WueRyEc-"
   },
   "source": [
    "Problem class to set up problems for the algorithms to be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eNaoAtoPmPM-"
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    self.f = None\n",
    "    self.grad_f = None\n",
    "    self.hessian = None\n",
    "    self.min_x = None\n",
    "\n",
    "  def himmelblau(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian(x):\n",
    "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
    "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian = hessian\n",
    "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
    "\n",
    "  def poly_1(self):\n",
    "\n",
    "    def f(x):\n",
    "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian(x):\n",
    "      return 3 * x**2 - 30 * x + 71\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian = hessian\n",
    "    self.min_x = np.array([[3],[5],[7]])\n",
    "\n",
    "  def rosenbrock(self):\n",
    "\n",
    "    def f(x):\n",
    "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(x):\n",
    "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "           200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian(x):\n",
    "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
    "           [-400*x[0], 200]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian = hessian\n",
    "    self.min_x = np.array([1,1])\n",
    "\n",
    "  #this is the function below rosenbrock defined in the project description\n",
    "  def func_2(self):\n",
    "\n",
    "    def f(x):\n",
    "      return 150*(x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return np.array([x[0] * (300 * x[1]**2 + 0.5) + 2 * x[1] - 2, 300*x[0]**2 * x[1] + 2 * x[0] + 8 * x[1] - 8])\n",
    "\n",
    "    def hessian(x):\n",
    "      return np.array([[300 * x[1]**2 + 0.5, 600 * x[0]*x[1] + 2],[600 * x[0]*x[1] + 2, 300 * x[0]**2 + 8]])\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian = hessian\n",
    "    #self calculated minima\n",
    "    self.min_x = np.array([[0,1],[4,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements a stopping criterion for when the gradient of f at xk is small relative to the gradient of f at x0\n",
    "Also a maxmimum number of iterations is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
    "  if i > max_iter: \n",
    "    return True\n",
    "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtracking linesearch to find suitable step length as described in the book. steepest_descent() implements the line search steepest descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
    "\n",
    "  alpha = alpha0\n",
    "\n",
    "  while not f(xk + alpha * pk) <= (f(xk) + c * alpha * grad_f(xk).T @ pk):\n",
    "    alpha *= rho\n",
    "  \n",
    "  return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wolfe Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf. newton_method() implements the line search Newton method solving the equation H @ pk = -grad instead of computing the inverse of H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
    "\n",
    "  alpha = 0\n",
    "  beta = np.Inf\n",
    "  t = 1\n",
    "\n",
    "  while True:\n",
    "\n",
    "    if f(xk + t * pk) > (f(xk) + c1 * t * (pk @ grad_f(xk))):\n",
    "      beta = t\n",
    "      t = 0.5 * (alpha + beta)\n",
    "    elif (-pk @ grad_f(xk + t * pk)) > (-c2 * pk @ grad_f(xk)):\n",
    "      alpha = t\n",
    "      t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
    "    else:\n",
    "      return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation (Taylor Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements the forward difference approach from the book for gradient and hessian calculation without explicit function knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "  arr = np.zeros(size)\n",
    "  arr[index] = 1.0\n",
    "  return arr\n",
    "\n",
    "#these return the function that computes the gradient, which can then in turn be called to compute the gradient\n",
    "def approx_grad(f, e=1.1e-8):\n",
    "  def grad_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + e) - f(x)) / e\n",
    "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "  return grad_f\n",
    "\n",
    "def approx_hessian(f, e=1.1e-8):\n",
    "  def hessian_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
    "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
    "  return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(x0, f, grad_f=None):\n",
    "\n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "  i = 0\n",
    "  xk = x0\n",
    "\n",
    "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
    "    \n",
    "    pk = -grad_f(xk)\n",
    "    xk = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
    "    i+=1\n",
    "    \n",
    "  print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
    "  return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(x0, f, grad_f=None, hessian_f=None):\n",
    "  \n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "    hessian_f = approx_hessian(f)\n",
    "  i = 0\n",
    "  x = x0\n",
    "  \n",
    "  while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
    "    \n",
    "    pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else solve(hessian_f(x), -grad_f(x))\n",
    "    x = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "    i += 1\n",
    "  \n",
    "  print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetcher Reeves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR(x0, f, grad_f=None):\n",
    "\n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "  i = 0\n",
    "  xk = x0\n",
    "  pk = -grad_f(xk)\n",
    "\n",
    "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
    "\n",
    "    xk1 = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
    "    beta = (grad_f(xk1) @ grad_f(xk1)) / (grad_f(xk) @ grad_f(xk))\n",
    "    pk = -grad_f(xk1) + beta * pk\n",
    "    xk = xk1\n",
    "    i += 1\n",
    " \n",
    "  print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
    "  return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SR1() implements the line search quasi newton method utilizing SR1 updating for inverse Hessian approximation and a method for stabilizing that resets H as a multiple of I inspired by the method deployed in https://www.sciencedirect.com/science/article/pii/S0898122111004202?via%3Dihub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR1(x0 : np.array, f, grad_f, r=1e-8):\n",
    "\n",
    "  conv_tol = 1e-8\n",
    "  i = 0\n",
    "  H = np.identity(x0.size)\n",
    "  x = x0\n",
    "\n",
    "  while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
    "  \n",
    "    pk = - H @ grad_f(x)\n",
    "    x_1 = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "    sk = x_1 - x\n",
    "    yk = grad_f(x_1) - grad_f(x)\n",
    "    rhok = 1 / (yk.T @ sk)\n",
    "    \n",
    "    if ((sk @ yk - yk @ H @ yk) < 0) or (abs(yk @ (sk - H @ yk)) < r * np.linalg.norm(yk) * np.linalg.norm(sk - H @ yk)) or (\n",
    "        norm(H, np.inf) > 1e10):\n",
    "      mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n",
    "      H_1 = mu * np.identity(x0.size)\n",
    "      H = H_1\n",
    "    else:\n",
    "      H_1 = H + np.outer((sk - H @ yk), (sk - H @ yk)) / ((sk - H @ yk) @ yk)\n",
    "      H = H_1\n",
    "    x = x_1\n",
    "    i += 1\n",
    "\n",
    "  print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKXNBar6zlfC",
    "outputId": "b491958b-75af-4e84-fdf7-9255325ce6e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output exact gradient: \n",
      "\n",
      " Search terminated after iteration 2816 with result: [-3.77931026 -3.28318599]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 2.6048138970693457e-07\n",
      "\n",
      "Difference to real solution: 2.5521830106979e-07\n",
      "\n",
      "Algorithm output approximated gradient: \n",
      "\n",
      " Search terminated after iteration 1922 with result: [-3.77931026 -3.283186  ]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 2.0923755687806018e-07\n",
      "\n",
      "Difference to real solution: 2.6212522690478664e-07\n",
      "\n",
      "Problem poly_1: \n",
      "\n",
      "\n",
      "Algorithm output exact gradient: \n",
      "\n",
      " Search terminated after iteration 221 with result: [7.00000006]\n",
      "\n",
      "actual minima: [[3]\n",
      " [5]\n",
      " [7]]\n",
      "\n",
      "Last gradient norm: 4.627252097937028e-07\n",
      "\n",
      "Difference to real solution: 5.784064871505734e-08\n",
      "\n",
      "Algorithm output approximated gradient: \n",
      "\n",
      " Search terminated after iteration 210 with result: [2.99999994]\n",
      "\n",
      "actual minima: [[3]\n",
      " [5]\n",
      " [7]]\n",
      "\n",
      "Last gradient norm: 4.6674944097515907e-07\n",
      "\n",
      "Difference to real solution: 4.000000063843677\n"
     ]
    }
   ],
   "source": [
    "#left this in to show how the problem class works etc.\n",
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output exact gradient: \")\n",
    "x_ = steepest_descent(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
    "print(\"\\nAlgorithm output approximated gradient: \")\n",
    "x_ = steepest_descent(np.array([0,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_1()\n",
    "print(f\"\\nProblem poly_1: \\n\")\n",
    "print(\"\\nAlgorithm output exact gradient: \")\n",
    "x_ = steepest_descent(np.array([1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")\n",
    "print(\"\\nAlgorithm output approximated gradient: \")\n",
    "x_ = steepest_descent(np.array([1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRYmj7uuDWmG"
   },
   "source": [
    "For the newton method I first struggled with solving the himmelblau function, but after some tinkering with the starting point, it worked.\n",
    "Also choosing 0s as starting points seems to not work here at all. In the end I reached VERY fast convergence for all problems though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9Dqi4AWCAp-",
    "outputId": "20d0671e-cc22-49bb-91c1-50c8049596cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 6 with result: [3. 2.]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 4.39446412658135e-14\n",
      "\n",
      "Difference to real solution: 1.4043333874306805e-15\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([2.5,1.5]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rqHHXq9EY3F"
   },
   "source": [
    "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8yFRBxNEitW",
    "outputId": "eea61628-81e1-4b48-9c5f-be39541c2436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 502 with result: [ 3.58442834 -1.84812652]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 2.5406398925923756e-07\n",
      "\n",
      "Difference to real solution: 6.237319089753625e-07\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkZV38tKGrel",
    "outputId": "e5b3ff83-ff39-4eff-b17c-26152a308d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 11 with result: [3. 2.]\n",
      "\n",
      "actual minima: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Last gradient norm: 1.3802086492309621e-07\n",
      "\n",
      "Difference to real solution: 4.5270257313018665e-09\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSmu1kaRcFF1"
   },
   "source": [
    "**Rosenbrock [1.2,1.2] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1Skp5VncEqT",
    "outputId": "8fafb866-e85f-4b4a-935d-728c2368f6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem rosenbrock steepest descent [1.2,1.2]:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 5001 with result: [1.00225488 1.00455996]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 0.016306835034657012\n",
      "\n",
      "Difference to real solution: 0.005087017651117713\n",
      "\n",
      "Problem rosenbrock steepest descent [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 5001 with result: [1.00225053 1.00455368]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 0.017387939189828356\n",
      "\n",
      "Difference to real solution: 0.0050794602350955455\n",
      "\n",
      "Problem rosenbrock newton method [1.2,1.2]:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 8 with result: [1. 1.]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.4360392201267428e-11\n",
      "\n",
      "Difference to real solution: 1.7227828323731315e-13\n",
      "\n",
      "Problem rosenbrock newton method [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 7 with result: [0.99999669 0.99999338]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.7793616466006474e-11\n",
      "\n",
      "Difference to real solution: 7.396216495370038e-06\n",
      "\n",
      "Problem rosenbrock FR [1.2,1.2]:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 1031 with result: [1.00000028 1.00000055]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.20863559177819e-06\n",
      "\n",
      "Difference to real solution: 6.199506706177205e-07\n",
      "\n",
      "Problem rosenbrock FR [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n",
      "\n",
      " Search terminated after iteration 479 with result: [0.99999701 0.99999401]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 3.457225675382256e-07\n",
      "\n",
      "Difference to real solution: 6.699859016039294e-06\n",
      "\n",
      "Problem rosenbrock:\n",
      "\n",
      "Algorithm output SR1 [1.2,1.2]: \n",
      "\n",
      " Search terminated after iteration 24 with result: [1. 1.]\n",
      "\n",
      "actual minima: [1 1]\n",
      "\n",
      "Last gradient norm: 1.2700270809299206e-08\n",
      "\n",
      "Difference to real solution: 2.5271779361523775e-09\n",
      "\n",
      "Problem rosenbrock SR1 [1.2,1.2] approximated gradients:\n",
      "\n",
      "Algorithm output: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SR1() missing 1 required positional argument: 'grad_f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-964c76aff8dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nAlgorithm output: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSR1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nactual minima: {prob.min_x}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapprox_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: SR1() missing 1 required positional argument: 'grad_f'"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([1.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [1.2,1.2]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [1.2,1.2]: \")\n",
    "x_ = SR1(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([1.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im-apNvwUEsz"
   },
   "source": [
    "**Rosenbrock [-1.2,1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C0zGoM_s01no",
    "outputId": "6ee66bbe-5959-4d75-a387-6b501cbcf089"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.2,1]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [-1.2,1]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [-1.2,1] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [-1.2,1]: \")\n",
    "x_ = SR1(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [-1.2,1] approximated gradients:\")\n",
    "#for some reason SR1 with this starting point and approximated gradients seems to be running forever\n",
    "\"\"\"print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1.2,1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV80lbwbcjGM"
   },
   "source": [
    "**Rosenbrock [0,1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dmjbZ3ZchH-",
    "outputId": "e546f819-c5f1-4d0a-c3fe-c3485a72bc37"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [0.0,1.0]: \")\n",
    "x_ = SR1(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [0.0,1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jiq4MRUJcv2d"
   },
   "source": [
    "**Rosenbrock [-1,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlWrpjIIcwBj",
    "outputId": "db8418b0-98ec-43b5-ca11-e30adc28b8ea"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [-1.0,0.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [-1.0,0.0]: \")\n",
    "x_ = SR1(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [-1.0,0.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyH8r8IIc9hd"
   },
   "source": [
    "**Rosenbrock [0,-1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oInw3wzKc9Xk",
    "outputId": "62e12034-bc92-490a-dc30-efad82307b0f"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,-1.0]:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock FR [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"\\nProblem rosenbrock:\")\n",
    "print(\"\\nAlgorithm output SR1 [0.0,-1.0]: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
    "print(f\"\\nProblem rosenbrock SR1 [0.0,-1.0] approximated gradients:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ivj3CII8dacl"
   },
   "source": [
    "**Function 2 [-0.2,1.2] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvbNrGZKdaS4",
    "outputId": "a72d29a3-f606-4af3-b418-0a6d484b5149"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-0.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-0.2,1.2] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-0.2,1.2]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkJky8GNdo_N"
   },
   "source": [
    "**Function 2 [3.8,0.1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg-qTessdpI0",
    "outputId": "bd1d28c8-a583-4615-ee7e-b0e526ce2a1b"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([3.8,0.1]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [3.8,0.1] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([3.8,0.1]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziFsa2GHdzt8"
   },
   "source": [
    "**Function 2 [0,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJG0EmTndz1U",
    "outputId": "2cff35bd-3f79-421a-a726-c86370040edf"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "#This raises linalgerror for singular matrix\n",
    "\"\"\"print(f\"\\nProblem func_2 [0.0,0.0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\"\"\"\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,0.0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,0.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DQ1Zg6ZdK6F"
   },
   "source": [
    "**Function 2 [-1,0] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-m41Z1lwTrSe",
    "outputId": "6fa39d8a-7071-40aa-da33-d92283893947"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1,0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [-1,0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([-1,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [-1,0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([-1,0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sav7POtNd8v-"
   },
   "source": [
    "**Function 2 [0,-1] starting point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4chOrUdBhrVb",
    "outputId": "d40e7a9c-923f-4330-86cd-7d2e4f68c3ee"
   },
   "outputs": [],
   "source": [
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] newton method:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "#this raises linalgerror singular matrix\n",
    "\"\"\"print(f\"\\nProblem func_2 [0.0,-1.0] newton method approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\"\"\"\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] FR:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] FR approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.func_2()\n",
    "print(f\"\\nProblem func_2:\")\n",
    "print(\"\\nAlgorithm output SR1: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
    "print(f\"\\nProblem func_2 [0.0,-1.0] SR1 approximate gradient:\")\n",
    "print(\"\\nAlgorithm output: \")\n",
    "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
    "print(f\"\\nactual minima: {prob.min_x}\")\n",
    "grad = approx_grad(prob.f)\n",
    "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
    "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOh95c0NmZ2b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project1_phase2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
