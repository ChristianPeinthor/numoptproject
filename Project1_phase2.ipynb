{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kZ8wEbZnBnQ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Project 1 Phase 2**<br>\n",
        "<br>\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th style = \"text-align: left\">#</th>\n",
        "    <th style = \"text-align: left\">Name</th>\n",
        "    <th style = \"text-align: left\">Lastname</th>\n",
        "    <th style = \"text-align: left\">Matr Number</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">1</td>\n",
        "    <td style = \"text-align: left\">Christian</td>\n",
        "    <td style = \"text-align: left\">Peinthor</td>\n",
        "    <td style = \"text-align: left\">11815592</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">2</td>\n",
        "    <td style = \"text-align: left\">Michael</td>\n",
        "    <td style = \"text-align: left\">Weikl</td>\n",
        "    <td style = \"text-align: left\">1154652</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">2</td>\n",
        "    <td style = \"text-align: left\">Uros</td>\n",
        "    <td style = \"text-align: left\">Zivanovic</td>\n",
        "    <td style = \"text-align: left\">12032271</td>\n",
        "  </tr>\n",
        "  \n",
        "</table>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xn6NQAvZ2Rfu"
      },
      "source": [
        "# Table of contents\n",
        "* [Problemdefinition](#Problemdefinition)\n",
        "* [Basic functions](#Basic-functions)\n",
        "    * [Stopping Criterion](#Stopping-Criterion)\n",
        "    * [Backtracking](#Backtracking)\n",
        "    * [Wolfe Condition](#Wolfe-Condition)\n",
        "    * [Derivation (Taylor-Theorem)](Derivation-(Taylor-Theorem))\n",
        "* [Implementation of the algorithms](#Implementation-of-the-algorithms)\n",
        "    * [Steepest descent](#Steepest-descent)\n",
        "    * [Newthon method](#Newthon-method)\n",
        "    * [Fetcher Reeves](#Fetcher-Reeves)\n",
        "    * [SR1](#SR1)\n",
        "* [Executions](#Executions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KwrOzH2PmHIH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy \n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy.linalg import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5B2jliEk2Rfv"
      },
      "source": [
        "# Problem Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqP_WueRyEc-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Basic Problem class to set up problems for the algorithms to be solved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eNaoAtoPmPM-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Problem():\n",
        "    def f(self, x):\n",
        "        raise NotImplementedError(\"f() is not implemented for this Problem\")\n",
        "    \n",
        "    def grad_f(self, x):\n",
        "        raise NotImplementedError(\"grad_f() is not implemented for this Problem\")\n",
        "            \n",
        "    def hessian(self, x):\n",
        "        raise NotImplementedError(\"hessian() is not implemented for this Problem\")      \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "p2upqTmt2Rfw"
      },
      "source": [
        "## Polynomials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gCyHN4Rn2Rfw"
      },
      "outputs": [],
      "source": [
        " class PolynomialProblem(Problem):\n",
        "        @property\n",
        "        def actual_minima(self):\n",
        "            raise NotImplementedError(\"actual_minima() is not implemented for this Problem\")      \n",
        "\n",
        "\n",
        "############################################################\n",
        "#                Polynomial definitions\n",
        "############################################################\n",
        "class Himmelblau(PolynomialProblem):\n",
        "    def f(self, x):\n",
        "        return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
        "    \n",
        "    def grad_f(self, x):\n",
        "        return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
        "                         4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
        "    \n",
        "    def hessian(self, x):\n",
        "        return np.array([[12*x[0]**2 + 4*x[1] - 42, \n",
        "                          4*(x[1] + x[0])],\n",
        "                         [4*(x[1] + x[0]),\n",
        "                          12*x[1]**2 + 4*x[0] - 26]])\n",
        "    \n",
        "    @property\n",
        "    def actual_minima(self):\n",
        "        return np.array([[3,2], \n",
        "                         [-2.805118, 3.131312], \n",
        "                         [-3.779310, -3.283186], \n",
        "                         [3.584428, -1.848126]])\n",
        "\n",
        "    \n",
        "class Polynomial1(PolynomialProblem):\n",
        "    def f(self, x):\n",
        "        return ((x - 7)**2 * (x - 3)**2) / 4\n",
        "    \n",
        "    def grad_f(self, x):\n",
        "        return (x - 7) * (x - 5) * (x - 3)\n",
        "    \n",
        "    def hessian(self, x):\n",
        "        return 3 * x**2 - 30 * x + 71\n",
        "    \n",
        "    @property\n",
        "    def actual_minima(self):\n",
        "        return np.array([[3],[5],[7]])\n",
        "\n",
        "    \n",
        "class Rosenbrock(PolynomialProblem):\n",
        "    def f(self, x):\n",
        "        return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "    def grad_f(self, x):\n",
        "        return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
        "                         200*(x[1] - x[0]**2)])\n",
        "\n",
        "    def hessian(self, x):\n",
        "        return np.array([[-400*(x[1] - 3*x[0]**2) + 2, \n",
        "                          -400*x[0]],\n",
        "                         [-400*x[0], \n",
        "                          200]])\n",
        "    \n",
        "    @property\n",
        "    def actual_minima(self):\n",
        "        return np.array([1,1])\n",
        "\n",
        "\n",
        "#this is the function below rosenbrock defined in the project description\n",
        "class Function2(PolynomialProblem):\n",
        "    def f(self, x):\n",
        "        return 150*(x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
        "    \n",
        "    def grad_f(self, x):\n",
        "        return np.array([x[0] * (300 * x[1]**2 + 0.5) + 2 * x[1] - 2, 300*x[0]**2 * x[1] + 2 * x[0] + 8 * x[1] - 8])\n",
        "\n",
        "    def hessian(self, x):\n",
        "        return np.array([[300 * x[1]**2 + 0.5, 600 * x[0]*x[1] + 2],[600 * x[0]*x[1] + 2, 300 * x[0]**2 + 8]])\n",
        "\n",
        "    #self calculated minima\n",
        "    @property\n",
        "    def actual_minima(self):\n",
        "        return np.array([[0,1],[4,0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fXbUwFs52Rfw"
      },
      "source": [
        "## Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ohiQ6st42Rfx"
      },
      "outputs": [],
      "source": [
        "# Helper functions for squared error problems\n",
        "def normal_distributed_data_points(target_function, q: float, num_points: int):\n",
        "    # bring normal distributed value from range [0, 1] to  [-q, q]\n",
        "    return distributed_data_points(target_function=target_function, q=q, num_points=num_points,\n",
        "                                   distribution=lambda r: (np.random.normal(0, scale=2 * r) - r))\n",
        "\n",
        "\n",
        "def uniform_distributed_data_points(target_function, q: float, num_points: int):\n",
        "    return distributed_data_points(target_function=target_function, q=q, num_points=num_points,\n",
        "                                   distribution=lambda r: np.random.uniform(low=-r, high=r))\n",
        "\n",
        "\n",
        "def distributed_data_points(target_function, q: float, num_points: int, distribution):\n",
        "    result_list = []\n",
        "\n",
        "    for _ in range(num_points):\n",
        "        val = distribution(abs(q))\n",
        "        result_list.append((val, target_function(val)))\n",
        "\n",
        "    return result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WZSA7Uvh2Rfx"
      },
      "outputs": [],
      "source": [
        "class SquaredErrorProblem(Problem):\n",
        "    def __init__(self, target_function,\n",
        "                 q: int = 2, num_points: int = 100, degree: int = 5,\n",
        "                 random_seed: int = 1154652, distribution='uniform'):\n",
        "        if distribution == 'uniform':\n",
        "            self.data = uniform_distributed_data_points(target_function=target_function, q=q, num_points=num_points)\n",
        "        elif distribution == 'normal':\n",
        "            self.data = normal_distributed_data_points(target_function=target_function, q=q, num_points=num_points)\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Functionality not implemented for ditribution type '{distribution}'. Use 'uniform' or 'normal'.\")\n",
        "\n",
        "        self.coefficients = []\n",
        "        for j in range(num_points):\n",
        "            a_j = self.data[j][0]\n",
        "            c_j = np.reshape([a_j ** i for i in range(degree + 1)], (degree + 1, 1))\n",
        "            self.coefficients.append(c_j)\n",
        "\n",
        "        self.target_function = target_function\n",
        "    \n",
        "    def __residual(self, x):\n",
        "        c = np.squeeze(np.array(self.coefficients))\n",
        "        b = np.reshape(np.array([v for _, v in self.data]), (len(self.data), 1))\n",
        "\n",
        "        return (c @ x) - b\n",
        "\n",
        "    def f(self, x):\n",
        "        r = self.__residual(x)\n",
        "        return 1 / 2 * np.sum(r ** 2)\n",
        "\n",
        "    def grad_f(self, x):\n",
        "        r = self.__residual(x)\n",
        "        c = np.squeeze(np.array(self.coefficients))\n",
        "        return (r.T @ c).T\n",
        "\n",
        "    def hessian(self, x):\n",
        "        c = np.squeeze(np.array(self.coefficients))\n",
        "        return (c.T @ c).T\n",
        "#     def __residual(self, x, j):\n",
        "#         a_j = self.data[j][0]\n",
        "#         b_j = self.data[j][1]\n",
        "#         c_j = self.coefficients[j]\n",
        "\n",
        "#         return c_j.T @ x - b_j\n",
        "\n",
        "#     def f(self, x):\n",
        "#         return 1 / 2 * np.sum([self.__residual(x, j) ** 2 for j in range(len(self.data))])\n",
        "\n",
        "#     def grad_f(self, x):\n",
        "#         return np.sum([self.__residual(x, j) * self.coefficients[j] for j in range(len(self.data))], axis=0)\n",
        "\n",
        "#     def hessian(self, x):\n",
        "#         return np.sum([self.coefficients[j] @ self.coefficients[j].T for j in range(len(self.data))], axis=0)\n",
        "\n",
        "\n",
        "############################################################\n",
        "#              Squared Error Problem Definition\n",
        "############################################################\n",
        "\n",
        "class SinusSQEP(SquaredErrorProblem):\n",
        "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
        "                 random_seed: int = 1154652, distribution='uniform'):\n",
        "        super().__init__(lambda x: np.sin(x), q, num_points, degree, random_seed, distribution)\n",
        "\n",
        "\n",
        "class CosinusSQEP(SquaredErrorProblem):\n",
        "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
        "                 random_seed: int = 1154652, distribution='uniform'):\n",
        "        super().__init__(lambda x: np.cos(x), q, num_points, degree, random_seed, distribution)\n",
        "\n",
        "\n",
        "class SinusMinusCosinusSQEP(SquaredErrorProblem):\n",
        "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
        "                 random_seed: int = 1154652, distribution='uniform'):\n",
        "        super().__init__(lambda x: np.sin(x) - np.cos(x), q, num_points, degree, random_seed, distribution)\n",
        "\n",
        "\n",
        "class CubePlusSquareSQEP(SquaredErrorProblem):\n",
        "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
        "                 random_seed: int = 1154652, distribution='uniform'):\n",
        "        super().__init__(lambda x: x ** 3 + x ** 2, q, num_points, degree, random_seed, distribution)\n",
        "\n",
        "\n",
        "class SinusPlusCosinusSQEP(SquaredErrorProblem):\n",
        "    def __init__(self, q: int = 2, num_points: int = 100, degree: int = 5,\n",
        "                 random_seed: int = 1154652, distribution='uniform'):\n",
        "        super().__init__(lambda x: 3 * np.sin(x) + np.cos(x), q, num_points, degree, random_seed, distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "i8I8128S2Rfy"
      },
      "source": [
        "# Basic functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "h8-OiqeE2Rfy"
      },
      "source": [
        "## Stopping Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Ylhc_kX42Rfy"
      },
      "source": [
        "This implements a stopping criterion for when the gradient of f at xk is small relative to the gradient of f at x0\n",
        "Also a maxmimum number of iterations is implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lVYaqSTG2Rfy"
      },
      "outputs": [],
      "source": [
        "def stop_crit(grad_f, xk, x0, current_iteration, tol: float = 1e-8, max_iter: int = 100):\n",
        "    if current_iteration > max_iter:\n",
        "        return True\n",
        "    elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uFmue2sW2Rfy"
      },
      "source": [
        "## Backtracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "09q3o8CE2Rfz"
      },
      "source": [
        "Backtracking linesearch to find suitable step length as described in the book. steepest_descent() implements the line search steepest descent method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ixQT2OHO2Rfz"
      },
      "outputs": [],
      "source": [
        "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
        "    alpha = alpha0\n",
        "    f_xk = f(xk)\n",
        "    grad_f_xk = grad_f(xk)\n",
        "\n",
        "    while f(xk + alpha * pk) > (f_xk + c * alpha * grad_f_xk.T @ pk):\n",
        "        alpha *= rho\n",
        "\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ooAB7mGg2Rfz"
      },
      "source": [
        "## Wolfe Condition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "74odhVLy2Rfz"
      },
      "source": [
        "alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf. newton_method() implements the line search Newton method solving the equation H @ pk = -grad instead of computing the inverse of H."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "H_40TfzQ2Rfz"
      },
      "outputs": [],
      "source": [
        "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
        "    alpha = 0\n",
        "    beta = np.Inf\n",
        "    t = 1\n",
        "\n",
        "    while True:\n",
        "        grad_calc = grad_f(xk)\n",
        "        if f(xk + t * pk) > (f(xk) + c1 * t * (pk.T @ grad_calc)):\n",
        "            beta = t\n",
        "            t = 0.5 * (alpha + beta)\n",
        "        elif (-pk.T @ grad_f(xk + t * pk)) > (-c2 * pk.T @ grad_calc):\n",
        "            alpha = t\n",
        "            t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
        "        else:\n",
        "            return t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "O9sVrH1G2Rfz"
      },
      "source": [
        "## Derivation (Taylor Theorem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7Wd4R8lY2Rfz"
      },
      "source": [
        "This implements the forward difference approach from the book for gradient and hessian calculation without explicit function knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bKG5Lodi2Rf0"
      },
      "outputs": [],
      "source": [
        "def e_i(size, index):\n",
        "    arr = np.zeros(size)\n",
        "    arr[index] = 1.0\n",
        "    return arr\n",
        "\n",
        "\n",
        "# these return the function that computes the gradient, which can then in turn be called to compute the gradient\n",
        "def approx_grad(f, e=1.1e-8):\n",
        "    def grad_f(x):\n",
        "        if x.size == 1:\n",
        "            return (f(x + e) - f(x)) / e\n",
        "        return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
        "\n",
        "    return grad_f\n",
        "\n",
        "\n",
        "def approx_hessian(f, e=1.1e-8):\n",
        "    def hessian_f(x):\n",
        "        if x.size == 1:\n",
        "            return (f(x + 2 * e) - 2 * f(x + e) + f(x)) / e ** 2\n",
        "        return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
        "            x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
        "            x)) / e ** 2 for j in range(x.size)] for i in range(x.size)])\n",
        "\n",
        "    return hessian_f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "QSfgaA4U2Rf0"
      },
      "source": [
        "# Implementation of the algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wEZc2H_A2Rf0"
      },
      "source": [
        "## Steepest descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "K83Dqzb22Rf0"
      },
      "outputs": [],
      "source": [
        "def steepest_descent(x0, f, grad_f=None, initial_alpha: float = 1e-3, tolerance: float = 1e-8):\n",
        "    if grad_f == None:\n",
        "        grad_f = approx_grad(f)\n",
        "\n",
        "    i = 0\n",
        "    xk = x0\n",
        "    search_alpha, alpha, pk = None, None, None\n",
        "\n",
        "    while not stop_crit(grad_f, xk, x0, i, tol=tolerance):\n",
        "\n",
        "        if search_alpha is None:\n",
        "            search_alpha = initial_alpha\n",
        "            pk = -grad_f(xk)\n",
        "        else:\n",
        "            new_pk = -grad_f(xk)\n",
        "            search_alpha = alpha * (-pk.T @ pk) / (-new_pk.T @ new_pk)\n",
        "            pk = new_pk\n",
        "\n",
        "        alpha = backtracking_alpha(f, grad_f, xk, pk)\n",
        "        xk = xk + alpha * pk\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return xk, i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "lHVL6VYF2Rf0"
      },
      "source": [
        "## Newton method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hTLuC7922Rf0"
      },
      "outputs": [],
      "source": [
        "def newton_method(x0, f, grad_f=None, hessian_f=None, tolerance=1e-8):\n",
        "    if grad_f == None:\n",
        "        grad_f = approx_grad(f)\n",
        "        hessian_f = approx_hessian(f)\n",
        "    i = 0\n",
        "    x = x0\n",
        "\n",
        "    while not stop_crit(grad_f, x, x0, i, tol=tolerance):\n",
        "        pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else solve(hessian_f(x), -grad_f(x))\n",
        "        x = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
        "        i += 1\n",
        "\n",
        "    #print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
        "    return x, i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rAsOOnlu2Rf1"
      },
      "source": [
        "## Fletcher Reeves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_9ah-ZQT2Rf1"
      },
      "source": [
        "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uvLuagFE2Rf1"
      },
      "outputs": [],
      "source": [
        "def FR(x0, f, grad_f=None, tolerance=1e-8):\n",
        "\n",
        "    conv_tol = 1e-8\n",
        "    if grad_f == None:\n",
        "        grad_f = approx_grad(f)\n",
        "    i = 0\n",
        "    xk = x0\n",
        "    pk = -grad_f(xk)\n",
        "\n",
        "    while not stop_crit(grad_f, xk, x0, i, tol=tolerance):\n",
        "\n",
        "        xk1 = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
        "        beta = (grad_f(xk1).T @ grad_f(xk1)) / (grad_f(xk).T @ grad_f(xk))\n",
        "        pk = -grad_f(xk1) + beta * pk\n",
        "        xk = xk1\n",
        "        i += 1\n",
        " \n",
        "    #print(f\"\\n Search terminated after iteration {i} with result: {xk}\")\n",
        "    return xk, i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "J6_W8FsY2Rf1"
      },
      "source": [
        "## BFGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bW2hKGdv2Rf1"
      },
      "outputs": [],
      "source": [
        "def approx_hessian_bfgs(f_grad, B_k, x_1, x):\n",
        "    \"\"\"\n",
        "    Approximate Hessian based on first derivitive using previous values.\n",
        "    :param f_grad: first derivitive of f\n",
        "    :param B_k: current hessian\n",
        "    :param x_1: x_{k+1}\n",
        "    :param x: x_{k}\n",
        "    :return: approximated hessian\n",
        "    \"\"\"\n",
        "\n",
        "    s_k = x_1 - x\n",
        "    y_k = f_grad(x_1) - f_grad(x)\n",
        "\n",
        "    return B_k - (B_k * s_k * np.transpose(s_k) * B_k) / \\\n",
        "           (np.transpose(s_k) * B_k * s_k) + \\\n",
        "           (y_k * np.transpose(y_k)) / (np.transpose(y_k) * s_k)\n",
        "\n",
        "\n",
        "def BFGS(x0, f, grad_f=None, tolerance=1e-8):\n",
        "    iterations = 1000\n",
        "    tolerance = tolerance\n",
        "    if grad_f is None:\n",
        "        f_prime = approx_grad(f)\n",
        "    else:\n",
        "        f_prime = grad_f\n",
        "\n",
        "    H = np.eye(x0.shape[0])\n",
        "    x = x0\n",
        "    i = 0\n",
        "\n",
        "    for i in range(iterations):\n",
        "        f_x = f_prime(x)\n",
        "\n",
        "        if np.all(np.abs(f_x) < tolerance):\n",
        "            return x\n",
        "\n",
        "        x_old = np.copy(x)\n",
        "\n",
        "        direction = -H @ f_x\n",
        "        step_size = alpha_wolfe(f, f_prime, x, direction)\n",
        "        x = x +  step_size * direction\n",
        "\n",
        "        x = np.sum(x.reshape(x.shape[0], -1), 1)\n",
        "        H = approx_hessian_bfgs(f_prime, H, x, x_old)\n",
        "\n",
        "    return x, i\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "alwOYLFC2Rf1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5rTFtst72Rf1"
      },
      "source": [
        "## SR1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "q4AV7I5G2Rf2"
      },
      "source": [
        "SR1() implements the line search quasi newton method utilizing SR1 updating for inverse Hessian approximation and a method for stabilizing that resets H as a multiple of I inspired by the method deployed in https://www.sciencedirect.com/science/article/pii/S0898122111004202?via%3Dihub.\n",
        "\n",
        "We limited maximum iterations to 55 for this algorithm because after that point in some cases computations got really heavy and led to very long searches without significant result improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3W_Dmb5A2Rf2"
      },
      "outputs": [],
      "source": [
        "def SR1(x0 : np.array, f, grad_f, r=1e-8):\n",
        "\n",
        "    if grad_f == None:\n",
        "        grad_f = approx_grad(f)\n",
        "    conv_tol = 1e-8\n",
        "    i = 0\n",
        "    H = np.identity(x0.size)\n",
        "    x = x0\n",
        "\n",
        "    while not stop_crit(grad_f, x, x0, i, tol=conv_tol, max_iter=54):\n",
        "  \n",
        "        pk = - H @ grad_f(x)\n",
        "        x_1 = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
        "        sk = x_1 - x\n",
        "        yk = grad_f(x_1) - grad_f(x)\n",
        "        rhok = 1 / (yk.T @ sk)\n",
        "    \n",
        "        if ((sk @ yk - yk @ H @ yk) < 0) or (abs(yk @ (sk - H @ yk)) < r * np.linalg.norm(yk) * np.linalg.norm(sk - H @ yk)) or (\n",
        "            norm(H, np.inf) > 1e10):\n",
        "            mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n",
        "            H_1 = mu * np.identity(x0.size)\n",
        "            H = H_1\n",
        "        else:\n",
        "            H_1 = H + np.outer((sk - H @ yk), (sk - H @ yk)) / ((sk - H @ yk) @ yk)\n",
        "            H = H_1\n",
        "        \n",
        "        x = x_1\n",
        "        i += 1\n",
        "\n",
        "    #print(f\"\\n Search terminated after iteration {i} with result: {x}\")\n",
        "    return x, i\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Xx-i0vYb2Rf2"
      },
      "source": [
        "# Executions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VC5GOw-o2Rf2"
      },
      "outputs": [],
      "source": [
        "# Define list with all possible algorithms for the tests\n",
        "algorithms = {'Steepest descent': steepest_descent,\n",
        "              'Newthon method': newton_method,\n",
        "              'Fletcher Reeves': FR,\n",
        "              'SR1': SR1,\n",
        "              'BFGS': BFGS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RCUDDIq-2Rf2"
      },
      "source": [
        "## Rosenbrock Problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKXNBar6zlfC",
        "outputId": "f08f7caf-0252-4f8d-a473-f74c8bc98ebf",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********************************************\n",
            "Problem Rosenbrock starting point[1.2, 1.2]:\n",
            "**********************************************\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Steepest descent':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [1.08281611 1.18006617]\n",
            "Last gradient norm: 3.4643633243321017\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 0.19819771189288904\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Newthon method':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 6 with result: [0.99999637 0.99999274]\n",
            "Last gradient norm: 5.130305357442694e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 8.120916393507349e-06\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Fletcher Reeves':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [1.00710328 1.00790061]\n",
            "Last gradient norm: 2.871566770808062\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 0.0106243238689931\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'SR1':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 24 with result: [0.9999967  0.99999339]\n",
            "Last gradient norm: 4.5358003846665094e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 7.393690821081246e-06\n",
            "**********************************************\n",
            "Problem Rosenbrock starting point[-1.2, 1]:\n",
            "**********************************************\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Steepest descent':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [2.0411798  4.15961153]\n",
            "Last gradient norm: 7.757432125690304\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 3.326740207767321\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Newthon method':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 23 with result: [0.99999669 0.99999338]\n",
            "Last gradient norm: 4.546056903178997e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 7.39619433403795e-06\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Fletcher Reeves':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [1.40094235 1.95159496]\n",
            "Last gradient norm: 7.331639922493372\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 1.0326120946259927\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'SR1':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 55 with result: [0.99999627 0.99999252]\n",
            "Last gradient norm: 3.4129698532069505e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 8.35519626253589e-06\n",
            "**********************************************\n",
            "Problem Rosenbrock starting point[0, 1]:\n",
            "**********************************************\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Steepest descent':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [ 3.45172162 11.91464163]\n",
            "Last gradient norm: 4.545496885363698\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 11.18661435898931\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Newthon method':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 14 with result: [0.99999669 0.99999338]\n",
            "Last gradient norm: 4.543658062814793e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 7.396315290542874e-06\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Fletcher Reeves':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [ 7.83320212 61.38135434]\n",
            "Last gradient norm: 56.37874667368861\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 60.76677219941073\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'SR1':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 36 with result: [0.99999649 0.99999297]\n",
            "Last gradient norm: 4.6687341856456785e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 7.861991136592512e-06\n",
            "**********************************************\n",
            "Problem Rosenbrock starting point[-1, 0]:\n",
            "**********************************************\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Steepest descent':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [2.31747958 5.36981901]\n",
            "Last gradient norm: 3.466999310504741\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 4.5641067712331385\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Newthon method':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [-0.96181955  0.92986517]\n",
            "Last gradient norm: 2.2965059665717846\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 1.9630728020944417\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Fletcher Reeves':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [ 4.47947442 20.09230575]\n",
            "Last gradient norm: 41.07528211675356\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 19.40677410433189\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'SR1':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 26 with result: [0.99999345 0.99998688]\n",
            "Last gradient norm: 7.122143309664058e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 1.4669456825577886e-05\n",
            "**********************************************\n",
            "Problem Rosenbrock starting point[0, -1]:\n",
            "**********************************************\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Steepest descent':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [ 4.49788924 20.22843746]\n",
            "Last gradient norm: 11.631280536384024\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 19.54400256127217\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Newthon method':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 21 with result: [0.99999669 0.99999338]\n",
            "Last gradient norm: 4.546008305648267e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 7.3961977068807766e-06\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'Fletcher Reeves':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 101 with result: [1.07725328 1.09009583]\n",
            "Last gradient norm: 33.573908661572894\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 0.11868162183662868\n",
            "------------------------------------------------\n",
            "\tAlgorithm 'SR1':\n",
            "------------------------------------------------\n",
            "Search terminated after iteration 42 with result: [0.99999669 0.99999336]\n",
            "Last gradient norm: 4.115536010068043e-06\n",
            "Difference to real solutions:\n",
            "\tDifference to point '[1 1]' is 7.417260466448856e-06\n"
          ]
        }
      ],
      "source": [
        "algorithms_rosenbrock = {'Steepest descent': steepest_descent,\n",
        "              'Newthon method': newton_method,\n",
        "              'Fletcher Reeves': FR,\n",
        "              'SR1': SR1}\n",
        "polynomial_problems = {Rosenbrock(): [1.2,1.2], \n",
        "                       Rosenbrock(): [-1.2,1],\n",
        "                       Rosenbrock(): [0,1],\n",
        "                       Rosenbrock(): [-1,0],\n",
        "                       Rosenbrock(): [0,-1]}\n",
        "\n",
        "for prob, start_value in polynomial_problems.items():\n",
        "    \n",
        "    print(\"**********************************************\")\n",
        "    print(f\"Problem {type(prob).__name__} starting point{start_value}:\")\n",
        "    print(\"**********************************************\")\n",
        "    for algorithm_name, algorithm in algorithms_rosenbrock.items():\n",
        "        try:\n",
        "            print(\"------------------------------------------------\")\n",
        "            print(f\"\\tAlgorithm '{algorithm_name}':\")\n",
        "            print(\"------------------------------------------------\")\n",
        "            parameters = {'x0': np.array(start_value),\n",
        "                          'f': prob.f,\n",
        "                          'grad_f': None,\n",
        "                          'hessian_f': None}\n",
        "\n",
        "            # clean up parameters for generic method call\n",
        "            possible_parameters = algorithm.__code__.co_varnames\n",
        "            for p in list(parameters):\n",
        "                if p not in possible_parameters:\n",
        "                    parameters.pop(p)\n",
        "\n",
        "            x, iterations = algorithm(**parameters)\n",
        "\n",
        "            print(f\"Search terminated after iteration {iterations} with result: {x}\")\n",
        "            print(f\"Last gradient norm: {norm(prob.grad_f(x))}\")\n",
        "            print(\"Difference to real solutions:\")\n",
        "            print(f\"\\tDifference to point '{prob.actual_minima}' is {norm(x - prob.actual_minima)}\")\n",
        "        \n",
        "        except Exception as err:\n",
        "            print(err)\n",
        "            print(f\"Algorithm '{algorithm_name}' failed to find a solution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "y1-bPf7S2Rf3"
      },
      "source": [
        "## Squared Error Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "id": "czYlEsfj2Rf3"
      },
      "outputs": [],
      "source": [
        "sqep_problems = {SinusSQEP: {'q': 2, 'num_points': 100, 'degree': 5},\n",
        "                 CosinusSQEP: {'q': 2, 'num_points': 100, 'degree': 5}}\n",
        "\n",
        "for prob_class, prob_params in sqep_problems.items():\n",
        "\n",
        "    prob = prob_class(**prob_params)\n",
        "\n",
        "    prob_num_points = prob_params['num_points']\n",
        "    prob_q = prob_params['q']\n",
        "    prob_degree = prob_params['degree']\n",
        "\n",
        "    x_plot = [i / 10 for i in range(-prob_q * 10, prob_q * 10 + 1)]\n",
        "    y_true_plot = [prob.target_function(x) for x in x_plot]\n",
        "\n",
        "    print(\"**********************************************\")\n",
        "    print(f\"Problem {type(prob).__name__}:\")\n",
        "    print(\"**********************************************\")\n",
        "    for algorithm_name, algorithm in algorithms.items():\n",
        "        try:\n",
        "            print(\"------------------------------------------------\")\n",
        "            print(f\"\\tAlgorithm '{algorithm_name}':\")\n",
        "            print(\"------------------------------------------------\")\n",
        "\n",
        "            parameters = {'x0': np.reshape([0 for i in range(prob_degree + 1)], (prob_degree + 1, 1)),\n",
        "                          'f': prob.f,\n",
        "                          'grad_f': prob.grad_f,\n",
        "                          'hessian_f': prob.hessian}\n",
        "\n",
        "            # clean up parameters for generic method call\n",
        "            possible_parameters = algorithm.__code__.co_varnames\n",
        "            for p in list(parameters):\n",
        "                if p not in possible_parameters:\n",
        "                    parameters.pop(p)\n",
        "\n",
        "            coef, iterations = algorithm(**parameters)\n",
        "            print(f\"Search terminated after iteration {iterations} with result: {coef}\")\n",
        "            print(f\"Last gradient norm: {norm(prob.grad_f(coef))}\")\n",
        "\n",
        "            approximation_y = []\n",
        "            for i in range(len(x_plot)):\n",
        "                approximation_y.append(0)\n",
        "                for d in range(prob_degree):\n",
        "                    approximation_y[i] += coef[d] * x_plot[i] ** d\n",
        "\n",
        "            plt.plot(x_plot, y_true_plot, label=\"true function\")\n",
        "            plt.plot(x_plot, approximation_y, '-.', label=\"approximation\")\n",
        "            plt.title(f'{algorithm_name}')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as err:\n",
        "            print(err)\n",
        "            print(f\"Algorithm '{algorithm_name}' failed to find a solution.\")\n",
        "\n",
        "# def draw_graphs(degree : int = 5, ):\n",
        "#     x = np.reshape([0 for i in range(degree+1)], (degree+1, 1))\n",
        "\n",
        "#     fig, ax = plt.subplots(4, figsize=(10, 15))\n",
        "\n",
        "#     x_multipliers, steps = newton_descent(x, 1000)\n",
        "#     xs = [i/10 for i in range(-q*10, q*10 + 1)]\n",
        "\n",
        "#     y_true = [fun(x) for x in xs]\n",
        "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
        "\n",
        "#     ax[0].plot(xs, y_true, label=\"true function\")\n",
        "#     ax[0].plot(xs, y_approx,'-.', label=\"approximation\")\n",
        "#     ax[0].set(title=f'Newton method after {steps} steps')\n",
        "#     ax[0].legend()\n",
        "\n",
        "#     x_multipliers, steps = conjugate_gradient(x, 15000)\n",
        "\n",
        "#     y_true = [fun(x) for x in xs]\n",
        "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
        "\n",
        "#     ax[3].plot(xs, y_true, label=\"true function\")\n",
        "#     ax[3].plot(xs, y_approx, label=\"approximation\")\n",
        "#     ax[3].set(title=f'Conjugate gradient method after {steps} steps')\n",
        "#     ax[3].legend()\n",
        "\n",
        "#     x_multipliers, steps = steepest_descent(x, 15000)\n",
        "\n",
        "#     y_true = [fun(x) for x in xs]\n",
        "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
        "\n",
        "#     ax[1].plot(xs, y_true, label=\"true function\")\n",
        "#     ax[1].plot(xs, y_approx, label=\"approximation\")\n",
        "#     ax[1].set(title=f'Steepest descent after {steps} steps')\n",
        "#     ax[1].legend()\n",
        "    \n",
        "#     # converges extremely slowly\n",
        "#     \"\"\"x_multipliers, steps = SR1(x, 15000)\n",
        "\n",
        "#     y_true = [fun(x) for x in xs]\n",
        "#     y_approx = [evaluate_poly(x, x_multipliers, n+1) for x in xs]\n",
        "\n",
        "#     ax[2].plot(xs, y_true, label=\"true function\")\n",
        "#     ax[2].plot(xs, y_approx, label=\"approximation\")\n",
        "#     ax[2].set(title=f'SR1 after {steps} steps')\n",
        "#     ax[2].legend()\"\"\"\n",
        "\n",
        "#     fig.tight_layout()\n",
        "    \n",
        "# draw_graphs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "MAdvJmHb2Rf3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "pVpiLXlF2Rf3"
      },
      "source": [
        "# Old Code #####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYmj7uuDWmG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "For the newton method I first struggled with solving the himmelblau function, but after some tinkering with the starting point, it worked.\n",
        "Also choosing 0s as starting points seems to not work here at all. In the end I reached VERY fast convergence for all problems though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Dqi4AWCAp-",
        "outputId": "20d0671e-cc22-49bb-91c1-50c8049596cf",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Problem' object has no attribute 'himmelblau'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-18-34497d4e3939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhimmelblau\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nProblem himmelblau: \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nAlgorithm output: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Problem' object has no attribute 'himmelblau'"
          ]
        }
      ],
      "source": [
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([2.5,1.5]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rqHHXq9EY3F",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Implementation of the Fletcher Reeves nonlinear conjugate gradient method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8yFRBxNEitW",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[3])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkZV38tKGrel",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmu1kaRcFF1",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Rosenbrock [1.2,1.2] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Skp5VncEqT",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [1.2,1.2]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([1.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [1.2,1.2]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [1.2,1.2]: \")\n",
        "x_ = SR1(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [1.2,1.2] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([1.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im-apNvwUEsz",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Rosenbrock [-1.2,1] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0zGoM_s01no",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.2,1] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.2,1]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.2,1]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.2,1] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [-1.2,1]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [-1.2,1] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [-1.2,1]: \")\n",
        "x_ = SR1(np.array([-1.2,1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [-1.2,1] approximated gradients:\")\n",
        "#for some reason SR1 with this starting point and approximated gradients seems to be running forever\n",
        "\"\"\"print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-1.2,1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV80lbwbcjGM",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Rosenbrock [0,1] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dmjbZ3ZchH-",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,1.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [0.0,1.0]: \")\n",
        "x_ = SR1(np.array([0.0,1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [0.0,1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiq4MRUJcv2d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Rosenbrock [-1,0] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlWrpjIIcwBj",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [-1.0,0.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [-1.0,0.0]: \")\n",
        "x_ = SR1(np.array([-1.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [-1.0,0.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-1.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyH8r8IIc9hd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Rosenbrock [0,-1] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oInw3wzKc9Xk",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock steepest descent [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock newton method [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,-1.0]:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock FR [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"\\nProblem rosenbrock:\")\n",
        "print(\"\\nAlgorithm output SR1 [0.0,-1.0]: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")\n",
        "print(f\"\\nProblem rosenbrock SR1 [0.0,-1.0] approximated gradients:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(grad(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivj3CII8dacl",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Function 2 [-0.2,1.2] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvbNrGZKdaS4",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-0.2,1.2]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([-0.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-0.2,1.2] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-0.2,1.2]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkJky8GNdo_N",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Function 2 [3.8,0.1] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-qTessdpI0",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([3.8,0.1]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([3.8,0.1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [3.8,0.1] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([3.8,0.1]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziFsa2GHdzt8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Function 2 [0,0] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJG0EmTndz1U",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "#This raises linalgerror for singular matrix\n",
        "\"\"\"print(f\"\\nProblem func_2 [0.0,0.0] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,0.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\"\"\"\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([0.0,0.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,0.0] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,0.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DQ1Zg6ZdK6F",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Function 2 [-1,0] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m41Z1lwTrSe",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-1,0] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-1,0] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1,0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [-1,0] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([-1,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [-1,0] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([-1,0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sav7POtNd8v-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Function 2 [0,-1] starting point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4chOrUdBhrVb",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] steepest descent approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = steepest_descent(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] newton method:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f, prob.grad_f, prob.hessian)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "#this raises linalgerror singular matrix\n",
        "\"\"\"print(f\"\\nProblem func_2 [0.0,-1.0] newton method approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = newton_method(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\"\"\"\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] FR:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] FR approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = FR(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[1])}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.func_2()\n",
        "print(f\"\\nProblem func_2:\")\n",
        "print(\"\\nAlgorithm output SR1: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")\n",
        "print(f\"\\nProblem func_2 [0.0,-1.0] SR1 approximate gradient:\")\n",
        "print(\"\\nAlgorithm output: \")\n",
        "x_ = SR1(np.array([0.0,-1.0]), prob.f)\n",
        "print(f\"\\nactual minima: {prob.min_x}\")\n",
        "grad = approx_grad(prob.f)\n",
        "print(f\"\\nLast gradient norm: {norm(prob.grad_f(x_))}\")\n",
        "print(f\"\\nDifference to real solution: {norm(x_ - prob.min_x[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOh95c0NmZ2b",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project1_phase2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}